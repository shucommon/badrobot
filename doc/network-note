## SCSI/iSCSI/FC/SAS
"SAN" is a term for a network used to provide (block) storage access. It can be anything.

SCSI is a family of storage protocols that all share a common core but have more or less subtle differences (apart from the obvious, physical ones). Examples are Parallel SCSI (obsolete), SSA (obsolete), Fibre Channel, Firewire (obsolete), SAS, iSCSI.

SAS and Fibre Channel are two different SCSI variants that each define they're own physical protocols and thus, infrastructure.

iSCSI is another SCSI variant but instead of defining a physical implementation, it sits on top of TCP/IP which most often runs over Ethernet.

Since all three are SCSI implementations, there are ways to make them compatible which each other (by multi-protocol bridging) or at least use them on the same physical infrastructure (e.g. FCoE). Bridging is costly however and most often you use just a single flavor or sometimes multiple in parallel.

PS: Parallel SCSI (SPI) is ancient and long obsolete. It was the foundation for the modern variants though and logically, all things SCSI use the same command protocol.


https://www.icc-usa.com/insights/sas-sata-and-iscsi-a-tutorial/

Yesterday there was a great piece over at TechRepublic on the confusing intersection of SAS, SATA and iSCSI arrays. For anyone who has ever had a difficult time understanding and differentiating these technologies, this is a very valuable resource. Excerpts are cross-posted below.
SATA and SAS are storage interface and bus types designed to aid in the movement of data from one place to another. Think of SAS and SATA as different kinds of computer interfaces, such as PCI Express, but there are actually multiple components that make up the overall SAS architecture.

Initiators. The initiator is the SAS controller to which SAS expanders or targets can be connected.
Expanders. Expanders sit between initiators and targets, but can also connect to other expanders, as you can see in Figure A. Expanders are sort of like network switches in that they can direct traffic and they provide the ability to scale the SAS architecture beyond single port limits.
Targets. A target is either a SAS drive or a SATA disk. SATA disks can be connected to SAS expanders and initiators, but do not perform quite as well as SAS disks.
Each drive has its own host interface (SAS or SATA) which are connected to an initiator or an expander. The initiator, functioning as the system controller, communicates with the SSD using the appropriate commands as dictated by the host interface (again, SAS or SATA.)

In other words, the SSD functions no differently than a hard-disk in terms of how it the interfaces (SAS or SATA) work. It simply utilizes a different method of storing the data.

Now… let’s introduce iSCSI
It’s either hardware- or software-based and controls communications with the Ethernet-based storage array. By virtue of the fact that the system is using Ethernet for storage traffic, speeds of both 1 Gbps and 10 Gbps are supported. In an iSCSI storage network, SCSI commands originate on the host, at which point, they are encapsulated inside TCP/IP packets. These TCP/IP packets then traverse the Ethernet network just like any other network traffic. At the storage array side of the equation, these packets are received and the SCSI commands are extracted from the TCP/IP packet and passed on to the storage device for execution. Data that is transmitted back to the host undergoes this same encapsulation process.
Basically, iSCSI adds some extra processor load because of the encapsulation process which takes time. We can think of iSCSI as a “storage transportation method.” In that sense, they resemble SATA and SAS in dealing with how the data is moved around (vs. SSD which deals with how the data is stored.) While this adds resource consumption, the author notes that it also provides a good method to decouple storage from the host, allowing for the share of storage among multiple hosts, as shown below:
However, you might be wondering exactly how does iSCSI compare to SATA and SAS. The author begins to address this.
iSCSI is the storage transport but, at present, there is no such thing as an “iSCSI disk.” iSCSI arrays use SAS and SATA disks (which can also be SSDs) for storage, but the data is transported to hosts using iSCSI.
While SAS and SATA or networking interfaces, iSCSI is a networking protocol that can run across the infrastructure. In that sense, iSCSI is complementary, not contradictory, to SAS and SATA.


## EDC PHY AND PHYLESS SWITCH DESIGN DIFFERENCE
( https://bm-switch.com/index.php/blog/edc-phy-and-phyless-switch-design-difference/ )
Almost all modern bare metal switches are described as PHYless. What does it mean, what we need to consider?

We commonly interconnect network devices for distances below 100 meters with twisted pair or maybe 300 meters over OM3 multimode fiber for 10GBASE-SR. We can extend the span between devices to 10Km for with 10GBASE-LR, 40Km for 10GBase-ER or even 80Km for 10GBase-ZR. However, the signal path doesn’t end at the transceiver. The electronic engineer designing the circuit board must ensure that the signal received by the transceiver is transmitted to the switch ASIC (and vice versa) while maintaining signal integrity and that it a major challenge. Each of the signals traveling along the copper paths has to deal with hazards like electromagnetic interference (EMI), coupling and crosstalk, signal reflections, impedance mismatches, skew, attenuation and a host of other threats as they travel.

An SFP+ transceiver uses an electrical interface called SFI to communicate with the switching ASIC. Modern ASICs (such as Broadcom Trident and Intel Alta family) are capable of receiving and interpreting the SFI-signaled 10Gbps signal directly, such approach is called PHYless. However, if the copper path length is too long to keep signal quality at an acceptable level, we need to place PHY or SERDES, between the transceiver and the ASIC.

These chips can perform some or all of the functions:
    ( http://thenetworksherpa.com/hardware-signals-traces-and-circuit-boards/ )

    Clock recovery and re-timing (common to all chips)

    Full electronic dispersion compensation (EDC) – to deal with 10GBASE-LRM

    Electrical interface conversion (e.g. SFI to XFI, or SFI to KR)

    SERDES – Serial / Parallel conversion (e.g. single lane SFI to four-lane XAUI)

    PHY-layer sub-functions like the Physical Coding Sublayer (PCS).

XFI 和 SFI 接口系统设计
作者：刘 亮   应用工程师
( https://e2echina.ti.com/blogs_/b/signal_integrity_/archive/2015/08/27/xfi-sfi )

XFI(Ziffy音)和SFI是两个常见的10Gbps高速串行接口，都是连接ASIC芯片和光模块的电气接口。在传统光通信，数据交换机和服务器等上都可以找到些接口。两者之间有什么相似的地方？区别在哪里？我设计的系统接口是否满足标准要求？本博客将一一尝试介绍。

XFI和SFI的来源

XFI来源于XFP光模块标准的一部分，指的是连接ASIC芯片和XFP光模块的电气接口。XFP光模块标准定义于2002年左右，其内部的收和发方向都带有CDR电路。因此XFP模块尺寸比较大，功耗也比较大，这个对于需要多端口高密度的系统，比如数通交换机会是一个问题。为了解决这两个问题，2006年左右，SFP+光模块标准出来了，其内部没有CDR电路，相对于XFP模块，SFP+模块尺寸和功耗都变小了。对应SFP+的电气接口叫做SFI。

XFI接口先于SFI接口出现。电气特性上，由于SFP+模块内部没有CDR，可以预见SFI的电气特性要求会比XFI来的更严格一些，这个可以从接下来的介绍的眼图和抖动指标要求中可以清楚的看出来。
标准以及参考点

XFI接口的电气特性定义在INF-8077文档，SFI接口的电气特性定义在SFF8431文档。

对于电气特性要求，这两个标准都定义了A，B，C和D四个参考点。

A代表系统板上ASIC芯片高速信号输出，封装管脚的位置

B代表系统板电信号输出的位置，即来自A点的信号经过PCB走线以后到达光模块的电输入的位置

C代表系统板上接收来自光模块的电信号，信号输入的位置

D 代表系统板上ASIC芯片的高速信号输入，封装管脚的位置。即C点的信号经过PCB走线以后到达ASIC的电输入位置
                    D                              C' C
                    <----------------------------------   RX       XFP Module
                    <----------------------------------                 | 
                                                                       --- light
    ASIC/SerDes                                                        \ /
                                                                        | 
                                                             Driver
                    ---------------------------------->  TX  --\        | 
                    ---------------------------------->      --/       --- light
                                                                       \ /
                                                                        | 


## What the Heck is a TCAM
( http://www.enterprisenetworkingplanet.com/netsysm/article.php/3527301/On-Your-Network-What-the-Heck-is-a-TCAM.htm )
let's talk about TCAM hardware, Cisco SDM, and try to answer that elusive question: "why do I have to reboot my router to enable certain features, which in turn disables others?"
First, CAM stands for Content Addressable Memory. A CAM is a special type of memory; some would say the opposite of RAM. With normal computer memory (RAM) the operating system provides an address, and receives the data stored at the supplied address. With a CAM, the operating system supplies the data, and the CAM returns a list of addresses where the data is stored, if it finds any. Furthermore, a CAM searches the entire memory in one operation, so it is considerably faster than RAM.
CAMs are very expensive, so they aren't normally found in PCs. Even router vendors will sometimes skimp, opting to instead implement advanced software-based searching algorithms. Most commonly CAMs and TCAMs are found in network processing devices, including Intel IXP cards and various routers or switches.

The most commonly implemented CAMs are called binary CAMs. They search only for ones and zeros; a simple operation. MAC address tables in switches commonly get stored inside binary CAMs. You can bet that any switch capable of forwarding Ethernet frames at line-speed gigabit is using CAMs for lookups. If they were using RAM, the operating system would have to remember the address where everything is stored. With CAMs, the operating system can find what it needs in a single operation. In this case it's the switchport that data should be sent out, based on the given MAC address, i.e. the essence of a MAC table. Some older Cisco switches running CatOS even opted to call this table the CAM table, thereby causing great confusion across the land.

Finally, a TCAM is a Ternary CAM. This allows the operating system to match a third state, "X." The X state is a "mask," meaning its value can be anything. This lends itself well to networking, since netmasks (define) operate this way. To calculate a subnet address we mask the bits we don't care about, and then apply the logical AND operation to the rest. Routers can store their entire routing table in these TCAMs, allowing for very quick lookups.

Hardware can sometimes seem to work like magic, but it isn't always transparent. When configuring routers most people will run into a situation where enabling a new feature will require that the Cisco SDM (Switching Database Manager) template be changed. This template is actually a method Cisco uses to assign specific application to specific TCAM resources.

Some routers will allow you to manually specify how much TCAM space you want to allocate to a specific feature. Others aren't so nice. They make you choose from a few restrictive templates, which allocate the resources automatically based on a few predetermined settings. For example, on the Cisco 3750, we recently wanted to enable policy-based routing (PBR) to implement a layer 3 jail. The basic idea with template-only routers is that you have to choose where you want most of the optimizations, and compromise on the rest.

For this platform, there are four templates: default, routing, PBR, and VLAN. Each of these tries to allow for a bit more resources allocated to the specified task. For policy routing, we'd have to choose "routing" or "PBR," which in turn limits the amount of unicast MAC addresses (define) that can be held in TCAMs. Likewise, selecting a VLAN (define) template will make PBR impossible, but allow for more VLAN database information to be held in TCAMs. There are always compromises when we need to use more advanced features. Keeping true to the spirit of router operating systems, there are also some mysterious side effects when a new template is chosen. On our specific router, if the PBR template is chosen, the router will become unable to support VPN routing/forwarding tables (VRF). The next unfortunate gotcha is that with the IOS version that supports IPv6, you cannot even enable PBR. There is no template to allow both policy routing and IPv6.

Here are some more TCAM allocation examples. Just because, for instance, 8K is allocated to routing tables, this doesn't mean that you can only have a routing table of that size. There's always the fallback of process switching. Process switching means that everything will be done by the processor instead of in hardware (TCAMs). Processor intervention is not desirable, mostly because it is much slower than hardware lookups. Also, the processor is supposed to be used for things like sending logs to a syslog server and controlling SSH sessions. If a router doing process switching gets really busy, it may be unable to service your console access attempts.

Hardware is finite, and we always need more. More expensive routers don't always suffer from the constant struggle for TCAMs because they have enough to support most features that currently exist. Unfortunately, most companies won't want to purchase the latest and greatest router with seemingly endless TCAMs unless they can justify the added cost by showing a need for them right now. So we're stuck having to adjust TCAM allocations.

