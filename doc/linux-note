## linux training tips
    ctrl+z stop
    fg/bg continue
    ctrl+c exit

    cpulimit -l percentage -p pid  - limit a process cpu percentage

    fork - vfork( MMU-less system)
    pthread_create -> clone

    fork/vfork/clone -> do_fork(clone_flag)

    pid - tgid (top group pid, posix comp)

    syscall(_NR_gettid)
    top -H thread view

    ps - id wa

    LMbench

    pmap

    \time -v python xx.py
    major/minor page fault

    smem
    smem --pie=command
    smem --bar=command

    buffer - disk raw info
    cache

    /proc/pid/smaps

    meld

    cgroup
    cgexec
    ./a.out cgexec Memory:A

    nm -D --defined-only
    apt install dot
    ldd app  -> show elf dependence .so

## vim
    # refresh opening file
	You can use the :edit command, without specifying a file name, to reload the current file. If you have made modifications to the file, you can use :edit! to force the reload of the current file (you will lose your modifications).

    % keyEdit
    The % key can be used for the following:

    To jump to a matching opening or closing parenthesis, square bracket or a curly brace: ([{}])
    To jump to start or end of a C-style comment: /* */.
    To jump to a matching C/C++ preprocessor conditional: #if, #ifdef, #else, #elif, #endif.
    To jump between appropriate keywords, if supported by the ftplugin file, for example, between begin and end in a Pascal program.
    A nice keymap is:

    noremap % v%
    This way, whenever you type % you jump to the matching object, and you visually select all the text in between. It's useful for indenting a C/C++ method/class: go to opening/closing brace, and type =%

   # resize window
	You can use the :resize command or its shortcut :res to change the height of the window. To change the height to 60 rows, use:
	:resize 60

	You can also change the height in increments. To change the height by increments of 5, use:
	:res +5
	:res -5

	You can use :vertical resize to change the width of the current window. To change the width to 80 columns, use:
	:vertical resize 80

	You can also change the width in increments. To change the width by increments of 5, use:
	:vertical resize +5
	:vertical resize -5

	For a split window: You can use Ctrl-w + and Ctrl-w - to resize the height of the current window by a single row. For a vsplit window: You can use Ctrl-w > and Ctrl-w < to resize the width of the current window by a single column. Additionally, these key combinations accept a count prefix so that you can change the window size in larger steps. [e.g. Ctrl-w 10 + increases the window size by 10 lines]

	To resize all windows to equal dimensions based on their splits, you can use Ctrl-w =.
	To increase a window to its maximum height, use Ctrl-w _.
	To increase a window to its maximum width, use Ctrl-w |.
	To resize in different steps, you can create maps that will adjust the window size differently. For example to increase the window size by a factor of 1.5 and decrease the window size by 0.67, you can map this:
	nnoremap <silent> <Leader>+ :exe "resize " . (winheight(0) * 3/2)<CR>
	nnoremap <silent> <Leader>- :exe "resize " . (winheight(0) * 2/3)<CR>

	In Gvim and vim in terminals with mouse support, it is also possible to use the mouse to resize a window. Simply grab the statusline at the window border and drag it into the desired direction.

    # file browser - vim built in netrw
        :Explore - opens netrw in the current window
        :Sexplore - opens netrw in a horizontal split
        :Vexplore - opens netrw in a vertical split
        You can also snigger by typing :Sex to invoke a horizontal split.

    # toggle case
        Toggle case "HellO" to "hELLo" with g~ then a movement.
        Uppercase "HellO" to "HELLO" with gU then a movement.
        Lowercase "HellO" to "hello" with gu then a movement.

    # plugin using Vundle
        1. Set up Vundle:
            git clone https://github.com/VundleVim/Vundle.vim.git ~/.vim/bundle/Vundle.vim

        2. configure plugins:
            Put this at the top of your .vimrc to use Vundle. Remove plugins you don't need, they are for illustration purposes.
                set nocompatible              " be iMproved, required
                filetype off                  " required

                " set the runtime path to include Vundle and initialize
                set rtp+=~/.vim/bundle/Vundle.vim
                call vundle#begin()
                " alternatively, pass a path where Vundle should install plugins
                "call vundle#begin('~/some/path/here')

                " let Vundle manage Vundle, required
                Plugin 'VundleVim/Vundle.vim'

                " The following are examples of different formats supported.
                " Keep Plugin commands between vundle#begin/end.
                " plugin on GitHub repo
                Plugin 'tpope/vim-fugitive'
                " plugin from http://vim-scripts.org/vim/scripts.html
                " Plugin 'L9'
                " Git plugin not hosted on GitHub
                Plugin 'git://git.wincent.com/command-t.git'
                " git repos on your local machine (i.e. when working on your own plugin)
                Plugin 'file:///home/gmarik/path/to/plugin'
                " The sparkup vim script is in a subdirectory of this repo called vim.
                " Pass the path to set the runtimepath properly.
                Plugin 'rstacruz/sparkup', {'rtp': 'vim/'}
                " Install L9 and avoid a Naming conflict if you've already installed a
                " different version somewhere else.
                " Plugin 'ascenator/L9', {'name': 'newL9'}

                " All of your Plugins must be added before the following line
                call vundle#end()            " required
                filetype plugin indent on    " required
                " To ignore plugin indent changes, instead use:
                "filetype plugin on
                "
                " Brief help
                " :PluginList       - lists configured plugins
                " :PluginInstall    - installs plugins; append `!` to update or just :PluginUpdate
                " :PluginSearch foo - searches for foo; append `!` to refresh local cache
                " :PluginClean      - confirms removal of unused plugins; append `!` to auto-approve removal
                "
                " see :h vundle for more details or wiki for FAQ
                " Put your non-Plugin stuff after this line

        3. Install Plugins:
            Launch vim and run :PluginInstall
            To install from command line: vim +PluginInstall +qall

    # adjust window size
    Ctrl+W_ will maximize a window vertically.
    Ctrl+W| will maximize a window horizontally.
    So far as I'm aware, there is no way to restore the previous layout after these actions, but Ctrl+W= will resize all windows to equal sizes.
## show hardware info
    lshw lspci lsdev lscpu lsusb lsblk
## cmd for hw control and cofigure
    hdparm
    write to proc/dev/sys
    inb outb
    setpci
## system calls
    include/uapi/asm-generic/unistd.h
## /proc/meminfo
## !! fast redone a comand
## $! fast insert previous parameters

## grub - https://wiki.centos.org/HowTos/Grub2
    1. The Boot Menu is Automatically Created

    Do not attempt to manually edit the boot menu because it is automatically created from files within the /boot/ directory tree. However you can adjust the /etc/default/grub file that defines the general settings and the /etc/grub.d/40_custom file to add specific custom entries.

    The content of the /etc/default/grub file looks like this:

    GRUB_TIMEOUT=5
    GRUB_DEFAULT=saved
    GRUB_DISABLE_SUBMENU=true
    GRUB_TERMINAL_OUTPUT="console"
    GRUB_CMDLINE_LINUX="crashkernel=auto rhgb quiet"
    GRUB_DISABLE_RECOVERY="true"
    Kernel options that are common to each entry are defined on the GRUB_CMDLINE_LINUX line. For example, if you like to see the full detailed boot messages, delete rhgb quiet. If you like to see the standard boot messages, just delete rhgb. Changes to the setting is made effective by executing the following command:

    [root@host ~]# grub2-mkconfig -o /boot/grub2/grub.cfg
    Generating grub configuration file ...
    Found linux image: /boot/vmlinuz-3.10.0-229.14.1.el7.x86_64
    Found initrd image: /boot/initramfs-3.10.0-229.14.1.el7.x86_64.img
    Found linux image: /boot/vmlinuz-3.10.0-229.4.2.el7.x86_64
    Found initrd image: /boot/initramfs-3.10.0-229.4.2.el7.x86_64.img
    Found linux image: /boot/vmlinuz-3.10.0-229.el7.x86_64
    Found initrd image: /boot/initramfs-3.10.0-229.el7.x86_64.img
    Found linux image: /boot/vmlinuz-0-rescue-605f01abef434fb98dd1309e774b72ba
    Found initrd image: /boot/initramfs-0-rescue-605f01abef434fb98dd1309e774b72ba.img
    done
    On UEFI-based systems, the command will be grub2-mkconfig -o /boot/efi/EFI/centos/grub.cfg

    2. add yourslef custom boot entry
        a. edit /etc/grub.d/40_custom
        for example, add below content:
            menuentry 'Custom linux boot entry' {
                linux   /boot/vmlinuz-4.15.0-43-generic root=UUID=ac3520ce-3503-4169-       be71-5378ace9c098 ro  quiet splash $vt_handoff initcall_debug
                initrd  /boot/initrd.img-4.15.0-43-generic
            }
        b. grub-mkconfig -o /boot/grub/grub.cfg
        c. reboot, press shift

## kernel module
    a. a.c
    b. Makefile : obj-m := a.o
    c. make -C /lib/modules/$(uname -r)/build M=$PWD modules

    makefile:

        obj-m := hello.o

        all:
            make -C /lib/modules/$(shell uname -r)/build M=$(PWD) modules

        clean:
            make -C /lib/modules/$(shell uname -r)/build M=$(PWD) clean

## kernel source
    lsb_release -a to get codename
    git clone git://kernel.ubuntu.com/ubuntu/ubuntu-bionic.git

    Ubuntu
    Check version
    cat /etc/lsb-releases

## chown/chmod
    Use chown to change ownership and chmod to change rights.
    use the -R option to apply the rights for all files inside of a directory too.
    Note that both these commands just work for directories too. The -R option makes them also change the permissions for all files and directories inside of the directory.
    For example
        $ sudo chown -R username:group directory
    will change ownership (both user and group) of all files and directories inside of directory and directory itself.

    sudo chown username:group directory
    will only change the permission of the folder directory but will leave the files and folders inside the directory alone.

    Note that if you use chown <user>: <file> (Note the left-out group), it will use the default group for that user.
    If you want to change only the group, you can use:
        chown :<group> <file>

    # Make the current user own everything inside the folder (and the folder itself):
    sudo chown -R $USER ~/.blabla
    sudo chown -R username:group directory

## build kernel
    time make clean
    time make -j8 bzImage
    time make -j8 modules
    time make modules_install
    time make install

## debug vmlinux
    using scripts/extract-vmlinux to unzip vmlinuz
        $ sudo /usr/src/linux-headers-$(uname -r)/scripts/extract-vmlinux vmlinuz-$(uname -r) > vmlinux
        $ objdump -D vmlinux | less
    All the symbols and their starting address can be found in the file /boot/System.map-$(uname -r).

## resize virtualbox .vdi size
    remove attachment of the vdi file
    VBoxManage modifymedium "/Users/qianggu/VirtualBox VMs/ubuntu-18.04/ubuntu-18.04.vdi"  --resize 51200
    readd vdi
    https://linuxhint.com/increase-virtualbox-disk-size/

## passwordless ssh to another server
    copy user a in host A's .ssh/id_rsa.pub to b in host B's .ssh/authorized_keys

## change default shell - chsh:
    cat /etc/shells  list the available shells on your system
    chsh
    /bin/bash  Type in the path and name of your new shell.
    su - yourid  Type in su - and your userid to relog in to verify that everything works correctly.

## tmux
    Reference:
    https://github.com/tmux/tmux
    https://hackernoon.com/a-gentle-introduction-to-tmux-8d784c404340 (My configured prefix is ctrl+a instead of ctrl+b)

    C-b %  split left-right
    C-b "  split top-bottom
    C-b <arrow key>  navigate panel
    exit/ctrl-d to exit
    C-b c  create new window
    C-b p  previous window
    C-b n  next window
    C-b <number>  jump to specific window
    C-b d  detach
    C-b D  select which session to detach
    tmux ls  list running session
    tmux attach -t 0  connect to specific session
    tmux new -s session_name  create tmux with a specific session_name
    tmux rename-session -t 0 session_name  to rename a sessionn with new session_name
    C-b ?
    C-b z: make a pane go full screen. Hit C-b z again to shrink it back to its previous size
    C-b C-<arrow key>: Resize pane in direction of <arrow key>
    C-b ,: Rename the current window
    .tmux.conf

    Attach tmux session:
    tmux a -t qa_171
    Detach tmux session:
    Ctrl+a  d

## cscope/ctags/tagbar
    .bashrc  export CSCOPE_EDITOR=/usr/bin/vim   - to specify cscope vim
    .vimrc   let g:tagbar_ctags_bin = '/usr/bin/ctags'  - to specify ctags tagbar used

    Tagbar
    under ~/.vim folder, create autoload bundle
    download pathogen.vim under autoload
        https://raw.githubusercontent.com/tpope/vim-pathogen/master/autoload/pathogen.vim
    in .vimrc add
        call pathogen#infect()
        call pathogen#helptags()

    cd ~/.vim/bundle/
    git clone git://github.com/majutsushi/tagbar or
    git clone https://github.com/majutsushi/tagbar.git

## install homebrew
    /usr/bin/ruby -e "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)"

## adduser/usermod
    $ sudo groupadd mynewgroup
    $ usermod -a -G examplegroup exampleusername
    change user primary group
        $ usermod -g groupname username
    To view the groups the current user account is assigned to
        $ group
        $ id
    $ groups exampleusername
    $ id exampleusername
    create a new user and assign a group
        $ useradd -G examplegroup exampleusername
        $ adduser username group
    view all groups on system
        $ getent group
    remove user from sudo group
        $ sudo deluser user_name sudo
        $ sudo gpasswd -d user_name sudo

    chown
    Use chown to change ownership and chmod to change rights.
    use the -R option to apply the rights for all files inside of a directory too.
    Note that both these commands just work for directories too. The -R option makes them also change the permissions for all files and directories inside of the directory.
    For example
    sudo chown -R username:group directory
    will change ownership (both user and group) of all files and directories inside of directory and directory itself.
    sudo chown username:group directory
    will only change the permission of the folder directory but will leave the files and folders inside the directory alone.

    Note that if you use chown <user>: <file> (Note the left-out group), it will use the default group for that user.
    If you want to change only the group, you can use:
	chown :<group> <file>

## su sudo root
    $ su
        switch to root, keep old environment
    $ su -
        switch to root, and create new environment based on root ~/.bashrc
    sudo only permmit run one cmd, su start a new shell, run utill quit
    sudo's cmd is exectued as target user(root), but will be logged with name configured in /etc/sudoers file
    ubuntu bind the use of su, you can use "$ sudo su" to jump this
    if you want to enable root in system
        $ sudo passwd root
    execute cmd as another suer
        $ sudo -u another_suer
    $ sudo -s
        new shell will currnet user .bashrc
    $ sudo -i
        new shell will new user .bashrc
    $ sudo -i
        cancel sudo pass cache

    show password as Asterisks
        $ sudo visudo
            Defaults        env_reset,timestamp_timeout=0,pwfeedback

## shell program
	$ hex="0x12345678"
	$ result=$((hex << 1))
	$ printf "Result in hex notation: 0x%x\n" "$result"
	0x2468acf0
	
	if [ "$seconds" -eq 0 ]; then
    	$timezone_string="Z"
	elif [ "$seconds" -gt 0 ]; then
        $timezone_string=`printf "%02d:%02d" $seconds/3600 ($seconds/60)%60`
	else
		echo "Unknown parameter"
	fi

	Bash: Assign Output of Shell Command To Variable
		var=$(command-name-here)
		var=$(command-name-here arg1)
		var=$(/path/to/command)
		var=$(/path/to/command arg1 arg2)
		OR
		var=`command-name-here`
		var=`command-name-here arg1`
		var=`/path/to/command`
		var=`/path/to/command arg1 arg2`

## os name
	$ lsb_release -a
		# if no lsb module
		$sudo apt-get install lsb-core
    $ hostnamectrl

## tmux error
    sudo locale-gen "en_US.UTF-8"
    sudo  dpkg-reconfigure locales # choose en_US.UTF-8
    or edit: sudo vi /etc/locale.gen

## ftp login
    ftp ftp://username:password@my.domain.com

## network related
    # modify mac address
    $ /etc/init.d/networking stop
    or $ ifconfig eth0 down
    $ ifconfig eth0 hw ether 00:60:16:75:12:02
    $ /etc/init.d/networking start
    or $ ifconfig eth0 up

    # ifconfig
    sudo ifconfig eth0 10.62.34.52/24
    ifconfig eth0 netmask 255.255.255.224
    ifconfig eth0 broadcast 172.16.25.63
    ifconfig eth0 172.16.25.125 netmask 255.255.255.224 broadcast 172.16.25.63
    ifconfig eth0 mtu 1000

    # route
    sudo route add default gw 10.62.34.1

    # add dns
        vim /etc/resolv.conf.
        Add - nameserver 10.32.102.224
        Save the file.

    # show port
        netstate -a

    # Network Cards info
        lspci command : List all PCI devices.
        lshw command : List all hardware.
        dmidecode command : List all hardware data from BIOS.
        ifconfig command : Outdated network config utility.
        ip command : Recommended new network config utility.
        http://www.cyberciti.biz/faq/linux-list-network-cards-command/

    # ip_forward
        Check if IP Forwarding is enabled
        We have to query the sysctl kernel value net.ipv4.ip_forward to see if forwarding is enabled or not: Using sysctl:
        sysctl net.ipv4.ip_forward
        net.ipv4.ip_forward = 0
        cat /proc/sys/net/ipv4/ip_forward

    # Namespace
    https://www.toptal.com/linux/separation-anxiety-isolating-your-system-with-linux-namespace
    http://www.opencloudblog.com/?p=42
    https://lwn.net/Articles/580893/
    https://lwn.net/Articles/531381/
    http://www.opencloudblog.com/?p=66
    http://serverfault.com/questions/127636/force-local-ip-traffic-to-an-external-interface
    http://stackoverflow.com/questions/2734144/linux-disable-using-loopback-and-send-data-via-wire-between-2-eth-cards-of-one

    route add -net <net> netmask <netmask> gw <gateway>  --> ip r add <net>/<netmasklen> via <gateway>
    ifconfig                                          --> ip addr or just ip a
    ifconfig <interface> up/down                      --> ip link set dev <interface> up/down
    ifconfig <interface> <ip> netmask <netmask>       --> ip addr add <ip>/<masklen> dev <interface>
    netstat -rn                                       --> ip route or just ip r
    ip route show table local
    udhcpc -i eth1
        ip netns add <new namespace name>
    Then, you'd assign the interface to the namespace:
        ip link set <device name> netns <namespace name>
    attach it to namespace
        ip link set tap1 netns ns1
    set the ports to up
        ip netns exec ns1 ip link set dev tap1 up

    ip netns exec netns1 ip link list
    #ip netns exec netns1 ip link set dev lo u
    ip netns exec nstest ip addr add 10.0.0.2/24 dev veth-b

    In addition, physical devices (those connected to real hardware) cannot be assigned to namespaces other than the root.
    Instead, virtual network devices (e.g. virtual ethernet or veth) can be created and assigned to a namespace.

    Ip route iptables:
    Have a look in local routing table. With iproute2 tools installed do ip route show table local.
    As you can see, all packets destinated to your local IPs would never go thru NICs since they are marked as local.
    To force packets go via ethernet card remove the appropriate route (i.e. ip route delete 192.168.122.1 dev eth0 table local).
    To restore this route just set the interface down and up: the kernel would do the work to insert these routes.
    The trick is to use a set of dummy IP addresses to force the kernel into routing it through the wire,
    and NAT to change it back to the real IP address.
    Let eth0 and eth1 be the two ethernet cards; IP0 and IP1 its IP address;
    MAC0 and MAC1 its MAC address respectively. We will be using two dummy IP addresses: IP00 and IP11.
        arp -s IP00 MAC0
        arp -s IP11 MAC1
        ip route add IP00 dev eth1
        ip route add IP11 dev eth0
        iptables -t nat -A POSTROUTING -d IP11 -j SNAT --to-source IP00
        iptables -t nat -A POSTROUTING -d IP00 -j SNAT --to-source IP11
        iptables -t nat -A PREROUTING -d IP00 -j DNAT --to-destination IP0
        iptables -t nat -A PREROUTING -d IP11 -j DNAT --to-destination IP1
    Use the dummy IP addresses IP00 and IP11 instead of the real one.

    Ifconfig

    eth4      Link encap:Ethernet  HWaddr 00:60:48:48:8B:E0
              UP BROADCAST MULTICAST  MTU:1500  Metric:1
              RX packets:25908936 errors:0 dropped:0 overruns:679371 frame:14
              TX packets:39659170 errors:0 dropped:0 overruns:0 carrier:0
              collisions:0 txqueuelen:1000
              RX bytes:38604314640 (35.9 GiB)  TX bytes:59092163300 (55.0 GiB)
              Interrupt:88 Memory:920024e0000-920024fffff

    eth5      Link encap:Ethernet  HWaddr 00:60:48:48:8B:E1
              UP BROADCAST MULTICAST  MTU:1500  Metric:1
              RX packets:43018730 errors:0 dropped:0 overruns:1140472 frame:20
              TX packets:34436309 errors:0 dropped:0 overruns:0 carrier:0
              collisions:0 txqueuelen:1000
              RX bytes:64097907700 (59.6 GiB)  TX bytes:51310100410 (47.7 GiB)
              Interrupt:153 Memory:920024c0000-920024dffff

    frame counts only misaligned frames, it means frames with a length not divisible by 8. Because of that length is not a valid frame and it is simply discarded.
    Meanwhile errors counts CRC errors, too-short frames and too-long frames.
    overruns counts that times when there is fifo overruns, caused by the rate at which the buffer gets full and the kernel isn't able to empty it.
    At last, dropped counts things like unintended VLAN tags or receiving IPv6 frames when the interface is not configured for IPv6.

    You can see how many packets have been dropped on your machine with netstat -suna. Mine has dropped 918 packets so far apparently ("918 packet receive errors")

## mount nfs
    mount
        On PXE 36
        sudo service portmap start
        sudo service nfs-kernel-server start
        sudo service nfs-common start

    On SP
        mount -o port=2049,nolock,proto=tcp -t nfs 192.168.1.1:/home/share/guoq3 /mnt/guoq3/

## format code
    http://astyle.sourceforge.net/

    use "astyle" command to format each of your new files into standard format:
        astyle --style:ansi -sbCKm0pD plugin.cpp

## tar.gz
    http://www.howtogeek.com/248780/how-to-compress-and-extract-files-using-the-tar-command-on-linux/

    tar -czvf name-of-archive.tar.gz /path/to/directory-or-file
        -c: Create an archive.
        -z: Compress the archive with gzip.
        -v: Display progress in the terminal while creating the archive, also known as "verbose" mode. The v is always optional in these commands, but it's helpful.
        -f: Allows you to specify the filename of the archive.
    tar.xz
        tar xf file.tar.zx
        tar xf file.tar.gz

## diff
    Once you have the source trees, e.g.
        diff -ENwbur repos1/ repos2/
    Even better
        diff -ENwbur repos1/ repos2/  | kompare -o -
    and have a crack at it in a good gui tool :)
        -Ewb ignore the bulk of whitespace changes
        -N detect new files
        -u unified
        -r recurse

## apt-get
    /var/lib/apt/lists/
    /etc/apt/sources.list
    /var/cache/apt/archives

## count all lines of folders
    find . -name '*.c' | xargs wc -l

    ( find ./ -name '*.c' -print0 | xargs -0 cat ) | wc -l
    works on names with spaces, only outputs one number.

## Samba server
    First, make sure that you've installed Samba server.
        www.howtogeek.com/howto/ubuntu/install-samba-server-on-ubuntu/
    Install samba:
        sudo apt-get install samba
    Set a Samba password:
        sudo smbpasswd -a USERNAME.
    This command will generate a prompt for a password (substitute USERNAME with your username).

    Modify smb.conf:
    To share the home directories, open up smb.conf with the following command:
        sudo vim /etc/samba/smb.conf
    Find this section of the file, and make it match the following:
        [homes]
        comment = Home Directories
        browseable = yes
        ...
        valid users = %S
        ...
        writable = yes
    Now you should be able to map a drive on windows using the following share format:
        \\ubuntumachine\username

    Add share folder:
        [<floder name>]
        path = <folder path>
        valid users = username1, username2
        read only = no
        browsable = yes
        public = yes
        writable = yes

## make
    make
    -g -ggdb3

## Apply patch of a diff file
    1 create a diff file.
    2 patch -p[num] < patchfile
    patch [options] originalfile patchfile
    http://www.thegeekstuff.com/2014/12/patch-command-examples

## readelf

## linux tools
    ccache - https://ccache.samba.org/   a fast C/C++ compiler cache
    CFLAGS - remove O2, add  -d dgdb

    tree
        tree --charset ASCII

    # binary diff
    1. xxd + diff
        % xxd b1 > b1.hex
        % xxd b2 > b2.hex
    And then
        % diff b1.hex b2.hex
    or
        % vimdiff b1.hex b2.hex

    2. cmp
    cmp -l b1 b2
    cmp -l file1.bin file2.bin | gawk '{printf "%08X %02X %02X\n", $1, strtonum(0$2), strtonum(0$3)}'
    cmp -l file1.bin file2.bin | gawk '{printf "%08X %02X %02X\n", $1-1, strtonum(0$2), strtonum(0$3)}'
    cmp -l file1.bin file2.bin | mawk 'function oct2dec(oct,     dec) {for (i = 1; i <= length(oct); i++) {dec *= 8; dec += substr(oct, i, 1)}; return dec} {printf "%08X %02X %02X\n", $1, oct2dec($2), oct2dec($3)}'
        cmp -l file1.bin file2.bin |
        mawk 'function oct2dec(oct,    dec) {
                  for (i = 1; i <= length(oct); i++) {
                      dec *= 8;
                      dec += substr(oct, i, 1)
                  };
                  return dec
              }
              {
                  printf "%08X %02X %02X\n", $1, oct2dec($2), oct2dec($3)
              }'

    3. diff + xxd
        Try diff in the following combination of zsh/bash process substitution:
            diff -y <(xxd foo1.bin) <(xxd foo2.bin)
        Where:
            -y shows you differences side-by-side (optional).
            xxd is CLI tool to create a hexdump output of the binary file.
            Add -W200 to diff for wider output (of 200 characters per line).
            For colors, use colordiff as shown below.
                colordiff -y <(xxd foo1.bin) <(xxd foo2.bin)
        You can also use vimdiff, e.g.
            vimdiff <(xxd foo1.bin) <(xxd foo2.bin)


## dmidecode
    https://linux.die.net/man/8/dmidecode

    dmi table decoder
    dmidecode is a tool for dumping a computer's DMI (some say SMBIOS ) table contents in a human-readable format.
    This table contains a description of the system's hardware components,
    as well as other useful pieces of information such as serial numbers and BIOSrevision.

## iPerf
    https://iperf.fr/
    iPerf3 is a tool for active measurements of the maximum achievable bandwidth on IP networks.
    It supports tuning of various parameters related to timing, buffers and protocols (TCP, UDP, SCTP with IPv4 and IPv6).
    For each test it reports the bandwidth, loss, and other parameters.
    This is a new implementation that shares no code with the original iPerf and also is not backwards compatible.
    iPerf was orginally developed by NLANR/DAST. iPerf3 is principally developed by ESnet / Lawrence Berkeley National Laboratory.
    It is released under a three-clause BSD license.

## packetgen

## timezone
    $ rm /etc/localtime
    $ cd /etc
    $ ln -s /usr/share/zoneinfo/Asia/Shanghai localtime

## screen
    https://www.gnu.org/software/screen/manual/screen.html

## CRLF&LF
    Display CRLF as ^M:
        :e ++ff=unix
    Substitute CRLF for LF:
        :setlocal ff=unix
        :w
        :e

## markdown
    mkdocs serve -a 10.32.136.25:8000 -f /home/qiang/xxx-docs/mkdocs.yml

    install python pip
    $ wget https://bootstrap.pypa.io/get-pip.py
    $ python get-pip.py
    $ pip install mkdocs
    $ pip install mkdocs-material
    $ mkdocs -version
    $ mkdocs new my-project
    $ cd my-project
    $ mkdocs serve
    INFO    -  Building documentation...
    INFO    -  Cleaning site directory
    [I 160402 15:50:43 server:271] Serving on http://127.0.0.1:8000
    [I 160402 15:50:43 handlers:58] Start watching changes
    [I 160402 15:50:43 handlers:60] Start detecting changes

    $ mkdocs build

    How to update Docs into GitHub:

    Step1: Install tools at your environment (once and for all)
        "python" version 2.7
        "pip"  ---- suggest use below commands
            sudo wget https://bootstrap.pypa.io/get-pip.py  --no-check-certificat
            sudo python ./get-pip.py
        "mkdocs"
            sudo pip install --upgrade mkdocs
            sudo pip install mkdocs-material

    Step2: Get codes
        Repo:  ssh://git@eos2git.cec.lab.emc.com/Chameleon/chameleon-kit.git

    Step3: Change code:
        Suggest to use markdownpad to modify your code.

        Add the new document to docs/ folder
            cd docs/
            git add <newpage.md>
        Edit the mkdocs.yml file
            site_name: Docs
            pages:
            - Home: index.md
            - About: README.md
            - <New-Page-Title>: <newpage.md>
            theme: 'material'

    Step4: Verify at Local:
        start http server:
            $ mkdocs serve -a 10.32.136.25:8000 -f /home/qiang/xxx-docs/mkdocs.yml
        access it with Chrome:
            $ http://10.32.136.25:8000

## history timestamps
    How to see time stamps in bash history
    $ export HISTTIMEFORMAT="%d/%m/%y %T "
    or
    $ echo 'export HISTTIMEFORMAT="%d/%m/%y %T "' >> ~/.bashrc
    $ source ~/.bashrc

## ssh tunnel
    ssh -f -N -D <1080 user@remote-server.com>
    ssh -fN -D :1080 xxx@xxx
    export all_proxy=socks5://localhost:1080

## send message within shell
    $sudo wall -n hi
    $sudo echo "Let's go have lunch... ok?" > /dev/pts/4
    $write username tty

    mesg [y|n]
    Mesg controls the access to your terminal by others.
    It's typically used to allow or disallow other users to write to your terminal (see write(1)).

## shell script
    # for loop
    for i in {1..1000000}; do echo -n "$i:"; wget http://www.intel.com/cd/edesign/library/asmo-na/eng/$i.htm; done;

    recursive execute all files
    find . -type f | xargs dos2unix

## wget
    wget -r -p -np -k http://xxx.com/abc/
    -r, –recursive          specify recursive download
    -k, –convert-links      make links in downloaded HTML point to local files.
    -p, –page-requisites    (necessary page element) get all images, etc. needed to display HTML page.
    -np, –no-parent         don’t ascend to the parent directory.
    -nc                     breakpoint continue
    -o                      log

## sed
    insert one line in the specific line:
    sed -i '8i8 This is Line 8' FILE

    find /path/to/files -type f -exec sed -i 's/oldstring/new string/g' {} \;
    grep -rl matchstring somedir/ | xargs sed -i 's/string1/string2/g'

    insert operation
        #insert # at the beginning of file.txt
        $ sed 's/^/#/' file.txt

        #insert blank at the beginning of file.txt
        $ sed 's/^/ /' file.txt
        sed 'line-num-1,line-num-2s/^/    /' intel_eth_device_errors.h

        # replace string start with " with ;
        sed '79,133s/".*/;/g' intel_eth_device_errors.h

        sed -i '23,$s/^/\t/' filename

    To append after the pattern: (-i is for inplace replace). line1 and line2 are the lines you want to append(or prepend)
        sed -i '/pattern/a line1 line2' inputfile
    To prepend the lines before:
    sed -i '/pattern/i line1 line2' inputfile

## lxc
    lxc image list
    lxc image list ubuntu: | less
    lxc launch ubuntu:16.04 first
    lxc list
    lxc info first
    lxc config show first
    free -m
    lxc exec first -- free -m
    lxc config set first limits.memory 64MB
    lxc exec first -- free -m
    lxc exec first -- apt-get update
    lxc exec first -- apt-get dist-upgrade -y
    lxc exec first -- apt-get autoremove --purge -y
    lxc snapshot first clean
    lxc exec first -- rm -Rf /etc /usr
    lxc exec first -- bash
    lxc restore first clean
    lxc exec first -- bash
    lxc publish first/clean --alias clean-ubuntu
    lxc stop first
    lxc delete first
    lxc launch clean-ubuntu second
    lxc file pull second/etc/hosts .
    lxc file push hosts second/etc/hosts
    lxc file pull second/var/log/syslog - | less
    lxc delete --force second
    lxc image list images: | less
    lxc launch images:centos/7 third
    lxc exec third -- cat /etc/redhat-release
    lxc remote list
    lxc remote add tryit 2001:470:b368:1070:216:3eff:fec6:67dc --password=safe
    lxc list tryit:
    lxc image list tryit:
    lxc launch clean-ubuntu tryit:fourth

## Compile
    Archives - An archive (or static library) is simply a collection of object files stored as a single file.
    (An archive is roughly the equivalent of a Windows .LIB file.) When you provide an archive to the linker,
    the linker searches the archive for the object files it needs, extracts them,
    and links them into your program much as if you had provided those object files directly.

    Create an archive
        % ar cr libtest.a test1.o test2.o

    Link .a
        // wrong method
        % gcc -o app -L. -ltest app.o
        // right one
        % gcc -o app app.o -L. -ltest

    Shared library
    step 1, compile the objects that will make up the library using the -fPIC option to the compiler
        % gcc -c -fPIC test1.c
    Step 2 - combine the object files into a shared library, like this:
        % gcc -shared -fPIC -o libtest.so test1.o test2.o
    -L. -ltest will first search .so then .a in the same path.
    Use -static option to demand static archives, if can't find .so, error return
        % gcc -static -o app app.o -L. -ltest

    The program only search /lib /usr/lib for .so, use -Wl,-rpath to specify .so path
        % gcc -o app app.o -L. -ltest -Wl,-rpath,/usr/local/lib
    Another solution to this problem is to set the LD_LIBRARY_PATH environment variable.

    Position-Independent Code (PIC)
    PIC stands for position-independent code. The functions in a shared library may be loaded at different
    addresses in different programs, so the code in the shared object must not depend on the address (or
    position) at which it is loaded. This consideration has no impact on you, as the programmer, except that
    you must remember to use the -fPIC flag when compiling code that will be used in a shared library.

## xxd hexdump

## find
    find a pattern in files and rename them
    find ./inc-test/ -name "*.orig" -exec rename -f 's/.h.orig/.h/' {} +
    find . -name '*-GHBAG-*' -exec bash -c 'mv $0 ${0/GHBAG/stream-agg}' {} \;
        mv ./report-GHBAG-1B ./report-stream-agg-1B
        mv ./reoprt-GHBAG-0.5B ./report-stream-agg-0.5B
    ignore some path
        find some_path -path path_to_ignore -prune -o -name "*.mk"

## The command-line history
    Use the up and down key's to scroll through previously typed commands.
    Press [Enter] to execute them or use the left and right arrow keys to edit the command first.

    Also see history (below).
    The history command can be used to list Bash's log of the commands you have typed:
    This log is called the "history". To access it type:
        history n
    This will only list the last n commands. Type "history" (without options) to see the the entire history list.

    You can also type
        !n to execute command number n.
        !! to execute the last command you typed.
        !-n will execute the command n times before (in other words !-1 is equivalent to !!).
        !string will execute the last command starting with that "string" and
        !?string? will execute the last command containing the word "string". For example:
        !cd Will re-run the command that you last typed starting with "cd".
        "commandName !*" will execute the "commandName" with any arguments you used on your last command.
    This maybe useful if you make a spelling mistake, for example. If you typed:
        emasc /home/fred/mywork.java /tmp/testme.java
    In an attempt to execute emacs on the above two files this will obviously fail. So what you can do is type:
        emacs !*
    This will execute emacs with the arguments that you last typed on the command-line. In other words this is equivalent to typing:
        emacs /home/fred/mywork.java /tmp/testme.java

    Searching through the Command History ( CTRL-R )
    Use the CTRL-R key to perform a "reverse-i-search". For example, if you wanted to use the command you used the last time you used snort, you would type:
    CTRL-R then type "snort".
    What you will see in the console window is:
    (reverse-i-search)`':
    After you have typed what you are looking for, use the CTRL-R key combination to scroll backward through the history.
    Use CTRL-R repeatedly to find every reference to the string you've entered. Once you've found the command you're looking for, use [Enter] to execute it.
    Alternatively, using the right or left arrow keys will place the command on an actual command-line so you can edit it.

## Set environment permanently
    You need to add it to your ~/.profile or .bashrc file.
    export PATH=$PATH:/path/to/dir
    Temporary:
    export PATH=$PATH:/path/to/dir

## PS1
    Change / Setup bash custom prompt
    Prompt is control via a special shell variable. You need to set PS1, PS2, PS3 and PS4 variable.
    If set, the value is executed as a command prior to issuing each primary prompt.

    PS1 - The value of this parameter is expanded (see PROMPTING below) and used as the primary prompt string. The default value is \s-\v\$ .
    PS2 - The value of this parameter is expanded as with PS1 and used as the secondary prompt string. The default is >
    PS3 - The value of this parameter is used as the prompt for the select command
    PS4 - The value of this parameter is expanded as with PS1 and the value is printed before each command bash displays during an execution trace.
          The first character of PS4 is replicated multiple times, as necessary, to indicate multiple levels of indirection. The default is +

    How do I modify or change the prompt?
    Modifying the prompt is easy task. Just assign a new value to PS1 and hit enter key:
        My old prompt -> [vivek@105r2 ~]
        $ PS1="touch me : "
    So when executing interactively, bash displays the primary prompt PS1 when it is ready to read a command,
    and the secondary prompt PS2 when it needs more input to complete a command. Bash allows these prompt
    strings to be customized by inserting a number of backslash-escaped special characters that are decoded as follows:
        \a : an ASCII bell character (07)
        \d : the date in "Weekday Month Date" format (e.g., "Tue May 26")
        \D{format} : the format is passed to strftime(3) and the result is inserted into the prompt string;
            an empty format results in a locale-specific time representation. The braces are required
        \e : an ASCII escape character (033)
        \h : the hostname up to the first '.'
        \H : the hostname
        \j : the number of jobs currently managed by the shell
        \l : the basename of the shell's terminal device name
        \n : newline
        \r : carriage return
        \s : the name of the shell, the basename of $0 (the portion following the final slash)
        \t : the current time in 24-hour HH:MM:SS format
        \T : the current time in 12-hour HH:MM:SS format
        \@ : the current time in 12-hour am/pm format
        \A : the current time in 24-hour HH:MM format
        \u : the username of the current user
        \v : the version of bash (e.g., 2.00)
        \V : the release of bash, version + patch level (e.g., 2.00.0)
        \w : the current working directory, with $HOME abbreviated with a tilde
        \W : the basename of the current working directory, with $HOME abbreviated with a tilde
        \! : the history number of this command
        \# : the command number of this command
        \$ : if the effective UID is 0, a #, otherwise a $
        \nnn : the character corresponding to the octal number nnn
        \\ : a backslash
        \[ : begin a sequence of non-printing characters, which could be used to embed a terminal control sequence into the prompt
        \] : end a sequence of non-printing characters

## irq/smp_affinity
    SMP IRQ Affinity

    Background:
    Whenever a piece of hardware, such as disk controller or ethernet card,
    needs attention from the CPU, it throws an interrupt.  The interrupt tells
    the CPU that something has happened and that the CPU should drop what
    it's doing to handle the event.  In order to prevent mutliple devices from
    sending the same interrupts, the IRQ system was established where each device
    in a computer system is assigned its own special IRQ so that its interrupts
    are unique.
    Starting with the 2.4 kernel, Linux has gained the ability to assign certain
    IRQs to specific processors (or groups of processors).  This is known
    as SMP IRQ affinity, and it allows you control how your system will respond
    to various hardware events.  It allows you to restrict or repartition
    the work load that you server must do so that it can more efficiently do
    it's job.

    Obviously, in order for this to work, you will need a system that has more
    than one processor (SMP).  You will also need to be running a 2.4 or higher
    kernel.

    Some brief and very bare information on SMP IRQ affinity is provided in
    the kernel source tree of the 2.4 kernel in the file:

        /usr/src/linux-2.4/Documentation/IRQ-affinity.txt
    How to use it:

    SMP affinity is controlled by manipulating files in the /proc/irq/ directory.
    In /proc/irq/ are directories that correspond to the IRQs present on your
    system (not all IRQs may be available). In each of these directories is
    the "smp_affinity" file, and this is where we will work our magic.

    The first order of business is to figure out what IRQ a device is using.
    This information is available in the /proc/interrupts file.  Here's a sample:

     [root@archimedes /proc]# cat /proc/interrupts
                CPU0       CPU1       CPU2       CPU3
       0:    4865302    5084964    4917705    5017077    IO-APIC-edge  timer
       1:        132        108        159        113    IO-APIC-edge  keyboard
       2:          0          0          0          0          XT-PIC  cascade
       8:          0          1          0          0    IO-APIC-edge  rtc
      10:          0          0          0          0   IO-APIC-level  usb-ohci
      14:          0          0          1          1    IO-APIC-edge  ide0
      24:      87298      86066      86012      86626   IO-APIC-level  aic7xxx
      31:      93707     106211     107988      93329   IO-APIC-level  eth0
     NMI:          0          0          0          0
     LOC:   19883500   19883555   19883441   19883424
     ERR:          0
     MIS:          0


    As you can see, this is a 4 processor machine.  The first column (unlabelled)
    lists the IRQs used on the system.  The rows with letters (ie, "NMI", "LOC")
    are parts of other drivers used on the system and aren't really accessible
    to us, so we'll just ignore them.

    The second through fifth columns (labelled CPU0-CPU3) show the number of times
    the corresponding process has handled an interrupt from that particular IRQ.
    For example, all of the CPUs have handled roughly the same number of interrupts
    for IRQ 24 (around 86,000 with CPU0 handling a little over 87,000).

    The sixth column lists whether or not the device driver associated with the
    interrupt supports IO-APIC (see /usr/src/linux/Documentation/i386/IO-APIC.txt
    for more information).  The only reason to look at this value is that
    SMP affinity will only work for IO-APIC enabled device drivers.  For
    example, we will not be able to change the affinity for the "cascade"
    driver (IRQ 2) because it doesn't support IO-APIC.

    Finally, the seventh and last column lists the driver or device that is
    associated with the interrupt.  In the above example, our ethernet card
    (eth0) is using IRQ 31, and our SCSI controller (aic7xxx) is using IRQ 24.

## Finding a patch's kernel version with git
    What I did to double-check was:

        git log fs/namei.c

    to find the commit series by Al (obviously, you can do it other ways too,
    but that was the easy way). Then, when you find the commit  just do

        git name-rev 1f36f774b22a0ceb7dd33eca626746c81a97b6a5

    which gives us

        1f36f774b22a0ceb7dd33eca626746c81a97b6a5 tags/v2.6.34-rc1~195^2

    ie that commit is reachable from v2.6.34-rc1, not from any stable kernel.

    (Or alternatively, use "git describe", and get "v2.6.33-5088-g1f36f77",
    which means that it is v2.6.33 plus 5088 commits).

    And as usual, there are other ways. One particularly obscure one is to say

        git log --tags --source --author=viro fs/namei.c

    which basically says "show only commits by viro in fs/namei.c, start from
    all tags, and for each commit, show _which_ tag the commit was reached
    from". It's not perfect, but it does it in one go. With "--oneline", you'd
    have gotten a listing like

        3e297b6 v2.6.34-rc3 Restore LOOKUP_DIRECTORY hint handling in final lookup on open()
        781b167 v2.6.34-rc2 Fix a dumb typo - use of & instead of &&
        1f36f77 v2.6.34-rc2 Switch !O_CREAT case to use of do_last()
        def4af3 v2.6.34-rc2 Get rid of symlink body copying
        3866248 v2.6.34-rc2 Finish pulling of -ESTALE handling to upper level in do_filp_open()
        806b681 v2.6.34-rc2 Turn do_link spaghetty into a normal loop
        10fa8e6 v2.6.34-rc2 Unify exits in O_CREAT handling
	9e67f36 v2.6.34-rc2 Kill is_link argument of do_last()
	...

    so you see into which -rc the different patches from Al went.

                Linus

    git describe --contains f3a1ef9cee4812e2d08c855eb373f0d83433e34c


## gdb

    help
        help command                        Get help on a certain command
        apropos keyword                     Search help for a particular keyword

    Starting and Quitting
        gdb [-tui] [-c core] [exename]      (Unix Command) Start gdb on an executable or standalone;
                                            specify "-tui" to start the TUI GUI; specify "-c" with a
                                            corefile name to see where a crash occurred
        run [arg1] [arg2] [...]             Run the currently loaded program with the given command line arguments
        quit                                Exit the debugger
        file exename                        Load an executable file by name

    Breakpoints and Watchpoints
        break location                      Set a breakpoint at a location, line number, or file (e.g. "main", "5", or "hello.c:23")
        watch expression                    Break when a variable is written to
        rwatch expression                   Break when a variable is read from
        awatch expression                   Break when a variable is written to or read from
        info break                          Display breakpoint and watchpoint information and numbers
        info watch                          Same as info break
        delete num                          Delete a breakpoint or watchpoint by number

    Stepping and Running
        next                                Run to the next line of this function
        step                                Step into the function on this line, if possible
        stepi                               Step a single assembly instruction
        continue                            Keep running from here
        CTRL-C                              Stop running, wherever you are
        finish                              Run until the end of the current function
        advance location                    Advance to a location, line number, or file (e.g. "somefunction", "5", or "hello.c:23")
        jump location                       Just like continue, except jump to a particular location first.

    Examining and Modifying Variables
        display expression                  Display the value of a variable or expression every step of the program-the expression must make sense in the current scope
        info display                        Show a list of expressions currently being displayed and their numbers
        undisplay num                       Stop showing an expression identified by its number (see info display)
        print expression                    Print the value of a variable or expression
        printf formatstr expressionlist     Do some formatted output with printf() e.g. printf "i = %d, p = %s\n", i, p
        set variable expression             Set a variable to value, e.g. set variable x=20
        set (expression)                    Works like set variable

    Window Commands
        info win                            Shows current window info
        focus winname                       Set focus to a particular window bby name ("SRC", "CMD", "ASM", or "REG") or by position ("next" or "prev")
        fs                                  Alias for focus
        layout type                         Set the window layout ("src", "asm", "split", or "reg")
        tui reg type                        Set the register window layout ("general", "float", "system", or "next")
        winheight val                       Set the window height (either an absolute value, or a relative value prefaced with "+" or "-")
        wh                                  Alias for winheight
        set disassembly-flavor flavor       Set the look-and-feel of the disassembly. On Intel machines, valid flavors are intel and att

    Misc Commands
        RETURN                              Hit RETURN to repeat the last command
        backtrace/bt                        Show the current stack
        attach pid                          Attach to an already-running process by its PID
        info registers                      Dump integer registers to screen
        info all-registers                  Dump all registers to screen

## sshpass
    Use sshpass:
    sshpass -p "password" scp -r user@example.com:/some/remote/path /some/local/path
    or so the password does not show in the bash history
    sshpass -f "/path/to/passwordfile" scp -r user@example.com:/some/remote/path /some/local/path
    The above copies contents of path from the remote host to your local.

## setpci

## addr2line
    convert addresses into file names and line numbers.

## make usb installer
    You can use dd.
        sudo umount /dev/sd<?><?>
    where <?><?> is a letter followed by a number, look it up. Then, next:
        sudo dd bs=4M if=input.iso of=/dev/sd<?> conv=fdatasync
    where input.iso is the input file, and /dev/sd<?> is the USB device you're writing to
    (run lsblkto see all drives to find out what <?> is for your USB).
    This method is fast and has never failed me.

    EDIT: for those on a Mac ending up here, use lowercase for bs=4m:
        sudo dd if=inputfile.img of=/dev/disk<?> bs=4m && sync

    EDIT: If USB drive does not boot (this happened to me), it is because the target is a particular partition
    on the drive instead of the drive. So the target needs to be /dev/sdc and not dev/sdc <?>For me it was /dev/sdb.
    Reference: https://superuser.com/a/407327 and https://askubuntu.com/a/579615/669976

## git
    git rebase
        integrate changes from one branch into another
        https://git-scm.com/book/en/v2/Git-Branching-Rebasing

    git blame
    git-blame - Show what revision and author last modified each line of a file
    git show <commit-id>

    For generating the patches from the topmost commits from a specific sha1 hash:
        git format-patch -<n> <SHA1>
    The last 10 patches from head in a single patch file:
        git format-patch -10 HEAD --stdout > 0001-last-10-commits.patch

    git commit --amend [-m ...]
    git push

    delete a commit
    Careful: git reset --hard WILL DELETE YOUR WORKING DIRECTORY CHANGES.
    Be sure tostash any local changes you want to keep before running this command.
    Assuming you are sitting on that commit, then this command will wack it...
        git reset --hard HEAD~1
    The HEAD~1 means the commit before head.
    Or, you could look at the output of git log, find the commit id of the commit you want to back up to, and then do this:
        git reset --hard <sha1-commit-id>

    If you already pushed it, you will need to do a force push to get rid of it...
        git push origin HEAD --force
    However, if others may have pulled it, then you would be better off starting a new branch.
    Because when they pull, it will just merge it into their work, and you will get it pushed back up again.
    If you already pushed, it may be better to use git revert, to create a "mirror image" commit that will undo the changes.
    However, both commits will be in the log.

    FYI -- git reset --hard HEAD is great if you want to get rid of WORK IN PROGRESS.
    It will reset you back to the most recent commit, and erase all the changes in your working tree and index.

    Lastly, if you need to find a commit that you "deleted", it is typically present in git reflog unless you have garbage collected your repository.

    git-apply - Apply a patch to files and/or to the index

    git - simple guide:
    http://rogerdudler.github.io/git-guide/index.zh.html

    # somebodys submit log
    git log --author=bob

    git push origin master # master can change to any branch you want
    git remote add origin <server>, then you can push your change to the server you added
    git push origin <branch>
    git add <filename>
    git log --pretty=oneline

    ASCII art tree show branch
        git log --graph --oneline --decorate -all

    git log --name-status
    delete local change, change to remote master
        git fetch origin git reset --hard origin/master

    The Merge Option
    git checkout feature
    git merge master
    Or, you can condense this to a one-liner:
    git merge master feature

    git merge confict
    git mergetool
    It opens a GUI that steps you through each conflict, and you get to choose how to merge.
    Sometimes it requires a bit of hand editing afterwards, but usually it's enough by itself.
    It is much better than doing the whole thing by hand certainly.
    Well, it doesn't necessarily open a GUI unless you install one.
    Running git mergetool for me resulted in vimdiff being used.
    You can install one of the following tools to use it instead:
    meld opendiff kdiff3 tkdiff xxdiff tortoisemerge gvimdiff diffuse ecmerge p4merge araxis vimdiff emerge

    Undo git add
    You can use 'git reset'. This will 'unstage' all the files you've added after your last commit.
    If you want to unstage only some files, use 'git reset -- <file 1> <file 2> <file n>'.
    Also it's possible to unstage some of the changes in files by using git reset -p

    Reversing patches
    You can reverse a patch if you have finished testing it, or if you want to see whether
    a problem has been introduced by a particular patch. You should also reverse a patch prior to adding a newer,
    updated version of the same patch. To reverse the patch, use the patch command with the -R option:
        patch -p1 -R < path/file.patch
    (If your patch was applied with the -p0 option, use that instead.)
    Or:
        git apply -R path/file.patch

    Git - show history of a file
        git log -p -- path/to/file
        gitk path/to/file
        git format-patch
    For generating the patches from the topmost commits from a specific sha1 hash:
        git format-patch -<n> <SHA1>
    The last 10 patches from head in a single patch file:
        git format-patch -10 HEAD --stdout > 0001-last-10-commits.patch

    # Keeping a fork repo up to date
        1. Clone your fork:
            git clone git@github.com:YOUR-USERNAME/YOUR-FORKED-REPO.git
        2. Add remote from original repository in your forked repository:
            cd into/cloned/fork-repo
            git remote add upstream git://github.com/ORIGINAL-DEV-USERNAME/REPO-YOU-FORKED-FROM.git
            git fetch upstream
        3. Updating your fork from original repo to keep up with their changes:
            git pull upstream master

    Colors in Git
    Git fully supports colored terminal output, which greatly aids in visually parsing command output quickly and easily.
    A number of options can help you set the coloring to your preference.
    color.ui
    Git automatically colors most of its output, but there's a master switch if you don't like this behavior.
    To turn off all Git's colored terminal output, do this:
        $ git config --global color.ui false

    Delete branch
    Delete Local Branch
    To delete the local branch use:
        $ git branch -d branch_name
    or use:
        $ git branch -D branch_name
    Note: The -d option is an alias for --delete, which only deletes the branch if it has already been fully merged in its upstream branch.
    You could also use -D, which is an alias for --delete --force, which deletes the branch "irrespective of its merged status." [Source: man git-branch]

    Delete Remote Branch
    As of Git v1.7.0, you can delete a remote branch using
        $ git push origin --delete <branch_name>
    which might be easier to remember than
        $ git push origin :<branch_name>

    Git history
        git log -- path/to/folder
        git log -- path/to/folder/*
    History of renamed files will not be followed with this method.
    if you want to use a graphic tool, such as gitk, it works the same:
        gitk -- path/to/folder

    You can use git log to display the diffs while searching:
        git log -p -- path/to/file
        git log -p will generate the a patch (the diff) for every commit selected. For a single file, use git log --follow -p $file.

## vim
    http://www.cyberciti.biz/faq/vim-text-editor-find-and-replace-all-text/
    Search and replace syntax
    The syntax is as follows:
        :s/Search/Replace/CommandFlag
        :s/Search-Word/Replace-Word/g
        :%s/Search-Word/Replace-Word/g
        :%s/Search-Word/Replace-Word/gc

    below sequence will delete the space between words
        w d ? \ s \ + <return>
    If your cursor is in the middle of some whitespace, 'diw' will delete whitespace left and right of the cursor.
    (If it is somewhere in the middle of a word, it will delete the word.)

    cscope
    sudo apt-get install vim-scripts
    sudo apt-get install exuberant-ctags
    copy .vim/plugin

    toggle case
    Toggle case "HellO" to "hELLo" with g~ then a movement.
    Uppercase "HellO" to "HELLO" with gU then a movement.
    Lowercase "HellO" to "hello" with gu then a movement.  select the word with visual mode (viw) and press ~, it switches case for all letters in the word.

    undo
    To undo recent changes, from normal mode use the undo command:
    u: undo last change (can be repeated to undo preceding commands)
    Ctrl-R: Redo changes which were undone (undo the undos).
    Compare to . to repeat a previous change, at the current cursor position.
    Ctrl-R (hold down Ctrl and press r) will redo a previously undone change, wherever the change occurred.

## IPMI
    ipmitool
    ipmitool -I lanplus -U admin -P Password1 -H 192.168.1.181 sol deactivate
    ipmitool -I lanplus -U admin -P Password1 -H 192.168.1.181 sol activate

    ipmitool -I lanplus -H <bmc ip address> -U admin -P Password1 chassis bootdev bios
    then ipmitool -I lanplus -H <bmc ip address> -U admin -P Password1 chassis power reset
    BMC eth0 (PCH NCSI):
    1.       Set to use static ip instead of DHCP
    a.       ipmi_cmd c 1 1 4 1
    2.       Set IP:
    a.       ipmi_cmd c 1 1 3 c0 a8 1 3       192.168.1.3
    BMC eth1 (1G lan)
    1.       Set to use static IP instead of DHCP
    a.       ipmi_cmd c 1 4 4 1
    2.       set IP:
    a.       ipmi_cmd c 1 4 3 c0 a8 1 3       192.168.1.3
    ipmitool mc info
    ipmitool -I lanplus -U admin -P Password1 -H 192.168.1.x command
    ipmitool fru
    ipmitool sel list	- get sel log

## big-little-endian
    The big end     31     23      15      7       0  the little end
                      01        02      03      04
                    byte1      byte2   byte3   byte4

            little endian                                           big endian
    31     23      15      7       0                         31     23      15      7       0
       01      02      03      04                               01       02     03      04
     byte1    byte2   byte3   byte4        byte address        byte1    byte2  byte3   byte4
       |        |       |       ^---------------Z----------------^        |      |       |
       |        |       -----------------------Z+1-------------------------      |       |
       |        -------------------------------Z+2--------------------------------       |
       ----------------------------------------Z+3----------------------------------------

## Comment style
    /* This is a comment */
    He's also happy with this format:

    /*
    * This is also a comment, but it can now be cleanly
    * split over multiple lines
    */

    If you must, use the comment format below and you'll avoid Torvalds' ire.
    // This can be a single line. Or many. Your choice.

    For those of you who like to provoke, Torvalds will tolerate this format:
    /* This is an alternate multi-line format
    that isn't horrible, but not kernel style */

    astyle --style=ansi -sbCKm0pD xxx.cpp

## book
    Linux kernel development - rober love -  Linux kernel version 2.6.34
    Professional linux kernel architecture - Wolfgang Mauerer - kernel 2.6.24
    Understanding the Linux Kernel- Daniel P. Bovet, Marco Cesati -  kernel 2.6.11

## screen
    $ screen /dev/cu.SLAB_USBtoUART 115200
    Ctrl+A then Ctrl+D. Doing this will detach you from the screen session which you can later resume by doing screen -r.
    You can also do: Ctrl+A then type :, this will put you in screen command mode.
    Type the command detach to be detached from the running screen session.
    $ screen -r
    $ ctrl+a then ctrl+\ to terminate
    $ screen ls

## uname
    -s kernel name
    -n computer name
    -r kernel release
    -v kernel version
    -m machine name
    -p processor type
    -i hardware platform
    -o operating system

    cat /proc/version
    cat /proc/cmdline
    cat /boot/config-$(uname -r)

## runlevel
    cat /var/run/utmp
    who -r
    runlevel

    change runlevel
        /sbin/telinit
        /sbin/init
        /var/initctl

## man ascii

## unicode - UTF-8
    If asked the question, "What is the difference between UTF-8 and Unicode?", would you confidently reply with a short and precise answer? In these days of internationalization all developers should be able to do that. I suspect many of us do not understand these concepts as well as we should. If you feel you belong to this group, you should read this ultra short introduction to character sets and encodings.

    Actually, comparing UTF-8 and Unicode is like comparing apples and oranges:

    UTF-8 is an encoding - Unicode is a character set

    A character set is a list of characters with unique numbers (these numbers are sometimes referred to as "code points"). For example, in the Unicode character set, the number for A is 41.

    An encoding on the other hand, is an algorithm that translates a list of numbers to binary so it can be stored on disk. For example UTF-8 would translate the number sequence 1, 2, 3, 4 like this:

    00000001 00000010 00000011 00000100 
    Our data is now translated into binary and can now be saved to disk.

    All together now
    Say an application reads the following from the disk:

    1101000 1100101 1101100 1101100 1101111 
    The app knows this data represent a Unicode string encoded with UTF-8 and must show this as text to the user. First step, is to convert the binary data to numbers. The app uses the UTF-8 algorithm to decode the data. In this case, the decoder returns this:

    104 101 108 108 111 
    Since the app knows this is a Unicode string, it can assume each number represents a character. We use the Unicode character set to translate each number to a corresponding character. The resulting string is "hello".

    Conclusion
    So when somebody asks you "What is the difference between UTF-8 and Unicode?", you can now confidently answer short and precise:

UTF-8 (Unicode Transformation Format) and Unicode cannot be compared. UTF-8 is an encoding used to translate numbers into binary data. Unicode is a character set used to translate characters into numbers.

## shutdown/poweroff/halt/reboot
    # shutdown
    shutdown schedules a time for the system to be powered down. It may be used to halt, power-off or reboot the machine.
    You may specify a time string (which is usually “now” or “hh:mm” for hour/minutes) as the first argument. Additionally, you may set a wall message to be sent to all logged-in users before the system goes down.
    Important: If the time argument is used, 5 minutes before the system goes down the /run/nologin file is created to ensure that further logins will not be allowed.
    Examples of shutdown commands:
        # shutdown
        # shutdown now
        # shutdown 13:20
        # shutdown -p now	#poweroff the machine
        # shutdown -H now	#halt the machine
        # shutdown -r09:35	#reboot the machine at 09:35am
    To cancel a pending shutdown, simply type the command below:
        # shutdown -c

    # halt
    halt instructs the hardware to stop all CPU functions, but leaves it powered on. You can use it to get the system to a state where you can perform low level maintenance.
    Note that in some cases it completely shuts down the system. Below are examples of halt commands:
        # halt		   #halt the machine
        # halt -p	   #poweroff the machine
        # halt --reboot    #reboot the machine

    # poweroff
    poweroff sends an ACPI signal which instructs the system to power down.
    The following are examples of poweroff commands:
        # poweroff   	       #poweroff the machine
        # poweroff --halt      #halt the machine
        # poweroff --reboot    #reboot the machine

    # reboot
    reboot instructs the system to restart.
        # reboot            #reboot the machine
        # reboot --halt     #halt the machine
        # reboot -p   	    #poweroff the machine

# linux boot
    https://www.kernel.org/doc/Documentation/arm/Booting

    the most typical setup is:

    1. CPU executes first-stage bootloader (FSB).
        FSB is located on the chip itself in ROM or EEPROM and is very small (AT91RM9200 FSB is 10kB max, AFAIR). FSB then initializes minimum set of peripherals (clocks, RAM, flash), transfers second-stage bootloader (U-Boot) to RAM, and executes it.

    2. U-Boot starts.
        U-Boot initializes some other hardware (serial, ethernet, etc), transfers Linux kernel to RAM, prepares the pointer to kernel input parameters and jumps into it's entry point.

    3. Linux kernel starts.
        Magic happens here. The system now able to serve you cookies via SSH console and/or executes whatever needs to be executed.

    The typical ARM processor in use today is integrated with peripherals on a single IC called a SoC, system on a chip. Typically the reboot procedure is nearly identical to a power-on boot procedure. On a reset the ARM processor typically jumps to address 0.

    Main memory, e.g. DRAM, and non-volatile storage, e.g. NAND flash, are typically external to the SoC (that is Linux capable) for maximum design flexibility.
    But typically there is a small (perhaps 128KB) embedded ROM (read-only memory) to initialize the minimal system components (e.g. clocks, external memories) to begin bootstrap operations. A processor reset will cause execution of this boot ROM. (This ROM is truly read-only, and cannot be modified. The code is masked into the silicon during chip fabrication.)
    The SoC may have a strapping option to instead execute an external boot memory, such as NOR flash or EEPROM, which can be directly executed (i.e. XIP, execute in place).
    The salient characteristic of any ROM, flash, and SRAM that the first-stage boot program uses is that these memories must be accessible immediately after a reset.

    One of the problems of bootstrapping a system that uses DRAM for main memory is its hardware initialization. The DRAM memory controller has to be initialized with board-specific parameters before code can be loaded into DRAM and executed. So from where does this board-specific initialization code execute, since it can't be in main memory?
    Each vendor has their own solution.
    Some require memory configuration data to be stored in nonvolatile memory for the boot ROM to access.
    Some SoCs have integrated SRAM (which does not require initialization like DRAM) to execute a small second-stage bootstrap program.
    Some SoCs use NOR flash to hold a XIP (execute in place) bootstrap program (e.g. the SPL program of U-Boot).

    Each SoC vendor has its own bootstrap method to get the OS loaded and executing.
    Some use hardware strapping read through GPIO pins to determine the source of the next stage of the bootstrap sequence.
    Another vendor may use an ordered list of memories and devices to probe for a bootstrap program.
    Another technique, is to branch to firmware in NOR flash, which can be directly executed (i.e. XIP, execute in place).

    Once the bootstrap program has initialized the DRAM, then this main memory can be used to load the next stage of booting. That could be a sophisticated boot utility such as U-Boot, or (if the bootstrap program is capable) the Linux kernel. A ROM boot program could do everything to load an ARM Linux kernel (e.g. ETRAX), but more common is that there will be several bootstrap programs or stages that have be performed between processor reset to execution of the OS.

    The requirements of booting the Linux ARM kernel are spelled out in the following document: Booting ARM Linux
    Older versions of Linux ARM used the ATAGs list to pass basic configuration information to the kernel. Modern versions provide a complete board configuration using a compiled binary of a Device Tree.
    ... e.g do boot loaders reinitialize DDR memory?

    Of the few examples that I have seen, the boot programs unconditionally configure the dynamic RAM controller.
    PCs have a BIOS and Power On Self Tests, aka POST. The execution of POST is the primary difference between a power-on reset (aka cold boot) versus a software reset (aka warm boot or reboot). ARM systems typically do not perform POST, so you typically will see minimal to no difference between types of reset.

# systemd
    hostnamectl  s
    timedatectl
    localectl
    loginctl session-status [id] / show-user user_name
    journalctl  -u sshd

# MBR GDP
    lsblk
    fdisk /dev/sdc
    parted /dev/sda print
    dd if=/dev/zero of=/dev/sdc count=1 bs=512
    GUID Partition table - GPT
    gdisk /dev/sdb
    grep CONFIG_EFI_PARTITION /boot/config-$(uname -r)

    UEFI    unified extensible firmware interface

    boot loader support:
        both BIOS and UEFI      UEFI Only       BIOS only
        GRUB 2                  Gummiboot       GRUB Legacy
        Syslinux                Clover          LILO

    GRUB Legacy:
        /boot/grub/grub.cfg
        /boot/grub/menu.lst

    GRUB shell
        > help
        > root (hd0,0)
        > kernel /vmlinuz-xxxx root=/dev/sda2 ro
        > initrd /intitramfs-xxx

    $ sudo grub
    grub > find /boot/grub/stage1
    grub > find /grub/stage1

    modify /etc/grub.d/40_custom
    $ grub2-mkconfig -o /boot/grub2/grub.cfg

# utils
    dnsmasq
    dig
    !$  -> last parameter

# top
    https://www.booleanworld.com/guide-linux-top-command/
    The top command allows users to monitor processes and system resource usage on Linux. It is one of the most useful tools in a sysadmin’s toolbox, and it comes pre-installed on every distribution. Unlike other commands such as ps, it is interactive, and you can browse through the list of processes, kill a process, and so on.

    Understanding top’s interface: the summary area
    As we have previously seen, top’s output is divided into two different sections. In this part of the article, we’re going to focus on the elements in half of the output. This region is also called the “summary area”.

    top - 23:05:27 up 189 days, 20:30,  1 user,  load average: 0.12, 0.44, 0.77
    Tasks: 380 total,   1 running, 347 sleeping,  32 stopped,   0 zombie
    Cpu(s):  0.1%us,  0.2%sy,  0.1%ni, 99.7%id,  0.0%wa,  0.0%hi,  0.0%si,  0.0%st
    Mem:  32874732k total, 29917900k used,  2956832k free,   444024k buffers
    Swap: 12582908k total,   205524k used, 12377384k free, 26595524k cached

     1682 root      20   0 63748 2344 1808 S  0.3  0.0 143:27.23 vmtoolsd
    38807 root      20   0  117m 4512 1656 S  0.3  0.0   0:20.77 timelines
    46722 qiang     20   0 28332 1844 1204 R  0.3  0.0   0:00.02 top
        1 root      20   0 30184 1548 1268 S  0.0  0.0   0:17.44 init


    System time, uptime and user sessions
    At the very top left of the screen (as marked in the screenshot above), top displays the current time. This is followed by the system uptime, which tells us the time for which the system has been running. For instance, in our example, the current time is “15:39:37”, and the system has been running for 90 days, 15 hours and 26 minutes.
    Next comes the number of active user sessions. In this example, there are two active user sessions. These sessions may be either made on a TTY (physically on the system, either through the command line or a desktop environment) or a PTY (such as a terminal emulator window or over SSH). In fact, if you log in to a Linux system through a desktop environment, and then start a terminal emulator, you will find there will be two active sessions.
    If you want to get more details about the active user sessions, use the who command.

    Memory usage
    The “memory” section shows information regarding the memory usage of the system. The lines marked “Mem” and “Swap” show information about RAM and swap space respectively. Simply put, a swap space is a part of the hard disk that is used like RAM. When the RAM usage gets nearly full, infrequently used regions of the RAM are written into the swap space, ready to be retrieved later when needed. However, because accessing disks are slow, relying too much on swapping can harm system performance.
    As you would naturally expect, the “total”, “free” and “used” values have their usual meanings. The “avail mem” value is the amount of memory that can be allocated to processes without causing more swapping.
    The Linux kernel also tries to reduce disk access times in various ways. It maintains a “disk cache” in RAM, where frequently used regions of the disk are stored. In addition, disk writes are stored to a “disk buffer”, and the kernel eventually writes them out to the disk. The total memory consumed by them is the “buff/cache” value. It might sound like a bad thing, but it really isn’t — memory used by the cache will be allocated to processes if needed.

    Tasks
    The “Tasks” section shows statistics regarding the processes running on your system. The “total” value is simply the total number of processes. For example, in the above screenshot, there are 27 processes running. To understand the rest of the values, we need a little bit of background on how the Linux kernel handles processes.
    Processes perform a mix of I/O-bound work (such as reading disks) and CPU-bound work (such as performing arithmetic operations). The CPU is idle when a process performs I/O, so OSes switch to executing other processes during this time. In addition, the OS allows a given process to execute for a very small amount of time, and then it switches over to another process. This is how OSes appear as if they were “multitasking”. Doing all this requires us to keep track of the “state” of a process. In Linux, a process may be in of these states:
    Runnable (R): A process in this state is either executing on the CPU, or it is present on the run queue, ready to be executed.
    Interruptible sleep (S): Processes in this state are waiting for an event to complete.
    Uninterruptible sleep (D): In this case, a process is waiting for an I/O operation to complete.
    Stopped (T): These processes have been stopped by a job control signal (such as by pressing Ctrl+Z) or because they are being traced.
    Zombie (Z): The kernel maintains various data structures in memory to keep track of processes. A process may create a number of child processes, and they may exit while the parent is still around. However, these data structures must be kept around until the parent obtains the status of the child processes. Such terminated processes whose data structures are still around are called zombies.
    Processes in the D and S states are shown in “sleeping”, and those in the T state are shown in “stopped”. The number of zombies are shown as the “zombie” value.

    CPU usage
    The CPU usage section shows the percentage of CPU time spent on various tasks. The us value is the time the CPU spends executing processes in userspace. Similarly, the sy value is the time spent on running kernelspace processes.
    Linux uses a “nice” value to determine the priority of a process. A process with a high “nice” value is “nicer” to other processes, and gets a low priority. Similarly, processes with a lower “nice” gets higher priority. As we shall see later, the default “nice” value can be changed. The time spent on executing processes with a manually set “nice” appear as the ni value.
    This is followed by id, which is the time the CPU remains idle. Most operating systems put the CPU on a power saving mode when it is idle. Next comes the wa value, which is the time the CPU spends waiting for I/O to complete.
    Interrupts are signals to the processor about an event that requires immediate attention. Hardware interrupts are typically used by peripherals to tell the system about events, such as a keypress on a keyboard. On the other hand, software interrupts are generated due to specific instructions executed on the processor. In either case, the OS handles them, and the time spent on handling hardware and software interrupts are given by hi and si respectively.
    In a virtualized environment, a part of the CPU resources are given to each virtual machine (VM). The OS detects when it has work to do, but it cannot perform them because the CPU is busy on some other VM. The amount of time lost in this way is the “steal” time, shown as st.

    Load average
    The load average section represents the average “load” over one, five and fifteen minutes. “Load” is a measure of the amount of computational work a system performs. On Linux, the load is the number of processes in the R and D states at any given moment. The “load average” value gives you a relative measure of how long you must wait for things to get done.
    Let us consider a few examples to understand this concept. On a single core system, a load average of 0.4 means the system is doing only 40% of work it can do. A load average of 1 means that the system is exactly at capacity — the system will be overloaded by adding even a little bit of additional work. A system with a load average of 2.12 means that it is overloaded by 112% more work than it can handle.
    On a multi-core system, you should first divide the load average with the number of CPU cores to get a similar measure.
    In addition, “load average” isn’t actually the typical “average” most of us know. It is an “exponential moving average“, which means a small part of the previous load averages are factored into the current value. If you’re interested, this article covers all the technical details.

    Understanding top’s interface: the task area
    PID
    This is the process ID, a unique positive integer that identifies a process.

    USER
    This is the “effective” username (which maps to a user ID) of the user who started the process. Linux assigns a real user ID and an effective user ID to processes; the latter allows a process to act on behalf of another user. (For example, a non-root user can elevate to root in order to install a package.)

    PR and NI
    The “NI” field shows the “nice” value of a process. The “PR” field shows the scheduling priority of the process from the perspective of the kernel. The nice value affects the priority of a process.

    VIRT, RES, SHR and %MEM
    These three fields are related with to memory consumption of the processes. “VIRT” is the total amount of memory consumed by a process. This includes the program’s code, the data stored by the process in memory, as well as any regions of memory that have been swapped to the disk. “RES” is the memory consumed by the process in RAM, and “%MEM” expresses this value as a percentage of the total RAM available. Finally, “SHR” is the amount of memory shared with other processes.

    S
    As we have seen before, a process may be in various states. This field shows the process state in the single-letter form.

    TIME+
    This is the total CPU time used by the process since it started, precise to the hundredths of a second.

    COMMAND
    The COMMAND column shows the name of the processes.

    Top command usage examples

    Killing processes
    If you want to kill a process, simply press ‘k’ when top is running. This will bring up a prompt, which will ask for the process ID of the process and press enter.
    Next, enter the signal using which the process should be killed. If you leave this blank, top uses a SIGTERM, which allows processes to terminate gracefully. If you want to kill a process forcefully, you can type in SIGKILL here. You can also type in the signal number here. For example, the number for SIGTERM is 15 and SIGKILL is 9.
    If you leave the process ID blank and hit enter directly, it will terminate the topmost process in the list. As we’ve mentioned previously, you can scroll using the arrow keys, and change the process you want to kill in this way.

    Sorting the process list
    One of the most frequent reasons to use a tool like top is to find out which process is consuming the most resources. You can press the following keys to sort the list:
        ‘M’ to sort by memory usage
        ‘P’ to sort by CPU usage
        ‘N’ to sort by process ID
        ‘T’ to sort by the running time

    You can also sort the list with the -o switch. For example, if you want to sort processes by CPU usage, you can do so with:
        top -o %CPU

    Showing a list of threads instead of processes
    We have previously touched upon how Linux switches between processes. Unfortunately, processes do not share memory or other resources, making such switches rather slow. Linux, like many other operating systems, supports a “lightweight” alternative, called a “thread”. They are part of a process and share certain regions of memory and other resources, but they can be run concurrently like processes.
    By default, top shows a list of processes in its output. If you want to list the threads instead, press ‘H’ when top is running. Notice that the “Tasks” line says “Threads” instead, and shows the number of threads instead of processes.
    If you want to switch back to the process view, press ‘H’ again. In addition, you can use the -H switch to display threads by default.

    Showing full paths
    By default, top does not show the full path to the program, or make a distinction between kernelspace processes and userspace processes. If you need this information, press ‘c’ while top is running. Press ‘c’ again to go back to the default.

    Forest view
    Sometimes, you may want to see the child-parent hierarchy of processes. You can see this with the forest view, by pressing ‘v’/’V’ while top is running.

    Listing processes from a user
    To list processes from a certain user, press ‘u’ when top is running. Then, type in the username, or leave it blank to display processes for all users.

    Filtering through processes
    If you have a lot of processes to work with, a simple sort won’t work well enough. In such a situation, you can use top’s filtering to focus on a few processes. To activate this mode, press ‘o’/’O’. A prompt appears inside top, and you can type a filter expression here.

    A filter expression is a statement that specifies a relation between an attribute and a value. Some examples of filters are:
        COMMAND=getty: Filter processes which contain “getty” in the COMMAND attribute.
        !COMMAND=getty: Filter processes which do not have “getty” in the COMMAND attribute.
        %CPU>3.0: Filter processes which have a CPU utilization of more than 3%.
    Once you’ve added a filter, you can further prune down things by adding more filters. To clear any filters you have added, press ‘=’.

    Changing the default look of the CPU and memory statistics
    If you are mostly at home in a GUI environment, you might not like top’s default way of showing CPU and memory statistics. You can press ‘t’ and ‘m’ to change the style of the CPU and memory statistics. Here is a screenshot of top, where we have pressed ‘t’ and ‘m’ once.

    Saving your settings
    If you’ve made any changes to top’s output, you can save them for later use by pressing ‘W’. top writes its configuration to the .toprc file in your home directory.

# install jupyter
    pip install --user jupyter matplotlib missingno notebook numpy pandas pytz scikit-learn scipy seaborn xlrd xgboost
    on mac:
    export PATH="$HOME/Library/Python/<version number>/bin:$PATH"

# tcpdump
    List interfaces that tcpdump can listen on
    $ tcpdump -D

    Turn on "verbose" key in TCPDUMP to see IP and TCP header information
    $ tcpdump -vi eth0

    Turn off hostname and port lookup in TCPDUMP
    $ tcpdump -vnni eth0

    Tcpdump filter only icmp traffic
    $ tcpdump -nni eth0 icmp

    Tcpdump command to filter on ICMP type - capture only ICMP echo request, ICMP echo requests are ICMP type 8
    $ tcpdump -nni vlan111 -e icmp[icmptype] == 8
    Above tcpdump filter "icmp[icmptype] == 8" will only display ip packets that have icmp payload and icmptype 8 - ICMP Echo Request.

    Tcpdump command to filter on ICMP type - capture only ICMP echo reply
    $ tcpdump -nni vlan111 -e icmp[icmptype] == 0

    Tcpdump filter packets with specified ip identification in ip header
    $ tcpdump -nr /tmp/tcpdump.pcap -v 'ip[4:2] == 24332'

    Tcpdump filtering based on DSCP field in IP header
    $ tcpdump -nni eth1 -v 'ip[1] & 0xfc == 184'

    Tcpdump: How to to capture only ICMP Fragmentation needed notifications
    $ tcpdump -nni vlan111 -e icmp[icmptype] == 3 && icmp[icmpcode] == 4

    How to capture frames with specific source or destination mac address
    $ tcpdump -nni eth0 ether src 2c:21:72:c6:c1:88
    $ tcpdump -nni eth0 ether dst 2c:21:72:c6:c1:88

    Capture only packets from a specific IP host or to a specific IP destination
    $ tcpdump -nni en0 src host 8.8.8.8.8
    $ tcpdump -nni en0 dst host 8.8.8.8.8

    Tcpdump - capture only ARP packets
    $ tcpdump -nni en0 arp

    Capture only IPv4 or only IPv6 traffic
    $ tcpdump -nni en0 ip
    $ tcpdump -nni en0 ip6

    Capture ethernet multicast traffic based on ethernet field and on IPv4 destination
    $ tcpdump -nni en0 "ether[0] & 1 != 0"
    $ tcpdump -nni en0 dst net 224.0.0.0/4

    Show ethernet / layer 2 headers
    $ tcpdump -nni en0 -e

    Capture only specific vlan traffic (for interfaces that terminate vlan trunks)
    $ tcpdump -nni em2 -e vlan 5

    Capture specific IPv4 protocols related traffic
    $ grep -E "esp|ah|gre|ospf|icmp|tcp|udp" /etc/protocols
    Showing below how to capture GRE traffic.
    $ tcpdump -nni em2 ip proto 47

    Dump HTTP data as ASCII or ASCII and HEX
    $ tcpdump -nni eth0 -s0 -A -l port 80
    $ tcpdump -nni eth0 -s0 -AX -l port 80

    Capture only traffic related to a CIDR subnet
    $ tcpdump -nni eth0 net 192.168.3.96/28

# In-Memory Layout of a Program (Process)
https://gabrieletolomei.wordpress.com/miscellanea/operating-systems/in-memory-layout/

In this post, I will try to explain how a program is actually laid out in main memory for being executed.
Here, I am assuming the program to be run on a multitasking Linux OS hosted on a 32-bit x86 architecture, in fact some of the details discussed below may be slightly different on other systems.

Each process in a multitasking OS runs in its own memory sandbox. This is the virtual address space, which in case of a 32-bit system is always a 4GB block of memory addresses. Indeed, since the smallest unit addressable by the CPU is 1 byte = 8 bit, it turns out that a 32-bit CPU is able to generate up to 2^32 – 1 “numbers”, i.e., addresses each one pointing to a specific byte in memory. These virtual addresses (generated by the CPU) are then mapped to real, physical memory addresses by page tables, which are kept by the operating system kernel. Since the OS kernel is a process itself, it has a dedicated portion of the virtual address space. This is separated from the portion reserved for any other user’s application processes (i.e., not OS processes). For instance, the first 3 GB of virtual addresses (i.e., from 0x00000000 to 0xBFFFFFFF) can be used for user processes while the last 1 GB of virtual addresses (i.e., from 0xC0000000 to 0xFFFFFFFF) are reserved to the OS kernel.

The following picture shows a possible split between OS kernel and user mode virtual memory spaces on a typical 32-bit Linux OS.
Please, refer to the post on Virtual Memory, Paging, and Swapping where I discuss better on this.

Coming back to our main subject, the picture below shows how a program looks like in main memory, in fact how a process looks like within the User Mode portion of the virtual address space! Let us inspect each memory segment, independently.

    – Text: Text segment, also known as a Code segment, is the section of memory which contains executable instructions of a program. It may be placed below the Heap or Stack in order to prevent overflows from overwriting it.
Usually, the text segment is sharable so that only a single copy needs to be in memory for frequently executed programs, such as text editors, the C compiler, the shells, and so on.
Finally, the text segment is often Read-only/Execute to prevent a program from accidentally modifying its instructions.

    – Data: Data segment is a shorthand for “Initialized data segment”.
This portion of the virtual address space of a program contains the global and static variables that are initialized by the programmer.
Generally speaking, the data segment is not Read-only, since the values of the variables can be altered at run time. In fact, it can be further classified into initialized Read-only area (i.e., RoData) and initialized Read-Write area.
For instance, the following C statements outside the main (i.e., global)
char s[] = "hello world";
int debug = 1;
would be stored in initialized Read-Write area.
On the other hand, a C statement like the following
const char* str = "hello world";
defines a character pointer variable (identified by str), which points to the first char of the constant string literal "hello world". In such a case, the string literal "hello world" is stored in initialized Read-only area while the character pointer variable str in initialized Read-Write area (because it may actually be modified at run-time). Similarly, static int i = 10; will also be stored in the Read-Write area of the data segment.

    – BSS: BSS segment also refers to “Uninitialized data segment”, and it is named so after an ancient assembler operator that stood for “Block Started by Symbol”. Data in this segment is initialized by the OS kernel to arithmetic 0 before the program starts executing.
Typically, this segment starts at the end of the data segment and contains all global and static variables that are initialized to zero or do not have explicit initialization in source code. For instance, a variable declared as static int i; would be allocated to the BSS segment.
Finally, the BSS segment is Read-Write.

    – Stack: The Stack area contains the program stack, i.e., a LIFO structure typically located in the higher memory addresses right below the OS kernel space (except for a constant offset called Random Stack Offset). On the standard x86 architecture it grows downwards to lower addresses, (i.e., as opposed to the adjacent Heap area which instead grows upwards) but on some other architectures it may grow the opposite direction.
This area is devoted to store all the data needed by a function call in a program. Specifically, the set of values pushed for one function call is named a stack frame, and consists of all the automatic variables (i.e., local to the scope of the function’s body and including any actual parameters passed as input to the function) and the caller’s return address. This is exactly how recursive functions are implemented in C: each time a recursive function calls itself, a new stack frame is allocated on top of the stack, thus the set of variables within one call are completely independent from those of another function call.
A stack pointer register tracks the top of the stack (i.e., how much of the stack area the process is currently using), and it is adjusted each time a value is “pushed” onto the stack. If the stack pointer meets the heap pointer (or if it eventually reaches the limit posed by RLIMIT_STACK), the available free memory is exhausted.

    – Heap: Heap is the segment where dynamic memory allocation usually takes place, i.e., to allocate memory requested by the programmer for variables whose size can be only known at run-time and cannot be statically determined by the compiler before program execution. The heap area begins at the end of the BSS segment and grows upwards to higher memory addresses. It is managed by malloc/new, free/delete, which may use the brk and sbrk system calls to adjust its size.
This area is shared by all shared libraries and dynamically loaded modules in a process.

# Virtual Memory, Paging, and Swapping
https://gabrieletolomei.wordpress.com/miscellanea/operating-systems/virtual-memory-paging-and-swapping/

                        ---------------------------  -------3.b invalid page request
                       |                          | /
              M  M-1  \/                        Page               2   1   0
            |-------------------|               Table            |-----------|
main memory |   |   |   |   |   | ...          ^   |        ...  |   |   |   |
            |-------------------|             /  / |             |-----------|
           /                   ^             /  /  |
         / 4. page swap        |            /  /   |---------------|
       /                       |2.b TLB miss  /                    |\5. page table update
     /             2.a TLB hit |          /  /3.a TLB update       |
   /                           |         /  /                      |/ 3.c page fault
  |        1. virtual address  |        / \/_                      |
  |  CPU ---------------------> MMU TLB                         Secondary
  |                                                             Storage
  |                                                                ^
  |----------------------------------------------------------------+


Virtual Memory is a memory management technique that is implemented using both hardware (MMU) and software (operating system). It abstracts from the real memory available on a system by introducing the concept of virtual address space, which allows each process thinking of physical memory as a contiguous address space (or collection of contiguous segments).
The goal of virtual memory is to map virtual memory addresses generated by an executing program into physical addresses in computer memory. This concerns two main aspects: address translation (from virtual to physical) and virtual address spaces management. The former is implemented on the CPU chip by a specific hardware element called Memory Management Unit or MMU. The latter is instead provided by the operating system, which sets up virtual address spaces (i.e., either a single virtual space for all processes or one for each process) and actually assigns real memory to virtual memory. Furthermore, software within the operating system may provide a virtual address space that can exceed the actual capacity of main memory (i.e., using also secondary memory) and thus reference more memory than is physically present in the system.

The primary benefits of virtual memory include freeing applications (and programmers) from having to manage a shared memory space, increasing security due to memory isolation, and being able to conceptually use more memory than might be physically available, using the technique of paging. Indeed, almost every virtual memory implementations divide a virtual address space into blocks of contiguous virtual memory addresses, called pages, which are usually 4 KB in size.
In order to translate virtual addresses of a process into physical memory addresses used by the hardware to actually process instructions, the MMU makes use of so-called page table, i.e., a data structure managed by the OS that store mappings between virtual and physical addresses.
Concretely, the MMU stores a cache of recently used mappings out of those stored in the whole OS page table, which is called Translation Lookaside Buffer (TLB).

The picture below describes the address translation task as discussed above.
When a virtual address needs to be translated into a physical address, the MMU first searches for it in the TLB cache (step 1. in the picture above). If a match is found (i.e., TLB hit) then the physical address is returned and the computation simply goes on (2.a.). Conversely, if there is no match for the virtual address in the TLB cache (i.e., TLB miss), the MMU searches for a match on the whole page table, i.e., page walk (2.b.). If this match exists on the page table, this is accordingly written to the TLB cache (3.a.). Thus, the address translation is restarted so that the MMU is able find a match on the updated TLB (1 & 2.a.).
Unfortunately, page table lookup may fail due to two reasons. The first one is when there is no valid translation for the specified virtual address (e.g., when the process tries to access an area of memory which it cannot ask for). Otherwise, it may happen if the requested page is not loaded in main memory at the moment (an apposite flag on the corresponding page table entry indicates this situation). In both cases, the control passes from the MMU (hardware) to the page supervisor (a software component of the operating system kernel). In the first case, the page supervisor typically raises a segmentation fault exception (3.b.). In the second case, instead, a page fault occurs (3.c.), which means the requested page has to be retrieved from the secondary storage (i.e., disk) where it is currently stored. Thus, the page supervisor accesses the disk, re-stores in main memory the page corresponding to the virtual address that originated the page fault (4.), updates the page table and the TLB with a new mapping between the virtual address and the physical address where the page has been stored (3.a.), and finally tells the MMU to start again the request so that a TLB hit will take place (1 & 2.a.).
As it turns out, the task of above works until there is enough room in main memory to store pages back from disk. However, when all the physical memory is exhausted, the page supervisor must also free a page in main memory to allow the incoming page from disk to be stored. To fairly determine which page to move from main memory to disk, the paging supervisor may use several page replacement algorithms, such as Least Recently Used (LRU). Generally speaking, moving pages from/to secondary storage to/from main memory is referred to as swapping (4.), and this is why page faults may occur.

Let us now turn back to the separation between OS kernel and user mode virtual memory spaces on a typical 32-bit Linux OS, as already introduced in the post In-Memory Layout of a Program (Process) and depicted in the following.
REPORT THIS AD

This does not mean the kernel uses that much physical memory. Instead, this is just the portion of virtual address space available to map whatever physical memory the OS kernel wishes, and it is orthogonal to the size of available physical memory. Note also that in Linux, kernel space is constantly present and maps the same physical memory in all processes, meaning that kernel space doesn’t change and is mapped to same physical memory addresses across any process context switch.
Following the above split rule, kernel has 1 GB kernel virtual address space dedicated, and whatever allocation it does, it uses always those set of addresses. The actual mapping of virtual addresses to physical memory addresses happens exactly as discussed above through a combination of hardware (i.e., MMU) and software (i.e., OS page supervisor).

Suppose the system X has 512 MB of physical memory, then only those 512 MB out of the entire 1 GB virtual space will be mapped for kernel address space, leaving the remaining 512 MB of virtual addresses unmapped. On the other hand, if X has 2 GB of physical memory, the entire 1 GB of virtual addresses will be mapped to physical addresses.
Having virtual memory could allow OS kernel pages to partly reside on secondary storage (i.e., disk) if the whole kernel does not fit to physical memory. In practice, this doesn’t happen (at least on Linux) since most recent Linux kernels need approximately only 70 MB, which is significantly below the amount of physical memory nowadays available on modern systems. Moreover, the kernel has data and code that must be kept always in main memory for efficiency reasons, and also because a page fault could not be handled otherwise. Think about what we discussed above when a page fault happens: the OS kernel (actually the OS page supervisor) takes the control of the system, enters a specific Interrupt Service Routine (ISR) to handle the page fault, and gives back the control to the user process that generated the page fault. If the OS kernel was not fit entirely to main memory, it might happen that the kernel itself generates a page fault. In a very bad case, such page fault could, for instance, concern the page with the code for the page fault handling routine, thereby blocking the whole system! That’s why kernel code and data are always addressable (i.e., it never generates a page fault), and ready to handle interrupts or system calls at any time.

# cpu info
    cpu cores number:
        $ cat /proc/cpuinfo | grep "cpu cores"
        cpu cores	: 8
        cpu cores	: 8
        cpu cores	: 8
        cpu cores	: 8
        cpu cores	: 8
        cpu cores	: 8
        cpu cores	: 8
        cpu cores	: 8
        cpu cores	: 8
        cpu cores	: 8
        cpu cores	: 8
        cpu cores	: 8
        cpu cores	: 8
        cpu cores	: 8
        cpu cores	: 8
        cpu cores	: 8

        $ cat /proc/cpuinfo | grep "cpu cores" | uniq
        cpu cores	: 8
    logical core number(if double cpu cores, means support HT):
        $ cat /proc/cpuinfo | grep siblings
        siblings	: 16
        siblings	: 16
        siblings	: 16
        siblings	: 16
        siblings	: 16
        siblings	: 16
        siblings	: 16
        siblings	: 16
        siblings	: 16
        siblings	: 16
        siblings	: 16
        siblings	: 16
        siblings	: 16
        siblings	: 16
        siblings	: 16
        siblings	: 16
    physical id(socket number):
        $ cat /proc/cpuinfo | grep "physical id"
        physical id	: 0
        physical id	: 0
        physical id	: 0
        physical id	: 0
        physical id	: 0
        physical id	: 0
        physical id	: 0
        physical id	: 0
        physical id	: 0
        physical id	: 0
        physical id	: 0
        physical id	: 0
        physical id	: 0
        physical id	: 0
        physical id	: 0
        physical id	: 0
        $ cat /proc/cpuinfo | grep "physical id" | sort | uniq
        physical id	: 0
        $ cat /proc/cpuinfo | grep "physical id" | sort | uniq  | wc -l
        1
    logical processor id(equal to siblings):
        $ cat /proc/cpuinfo | grep processor | wc -l
        16
