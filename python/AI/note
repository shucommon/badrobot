BP(back propagation) 神经网络

Sigmoid function
f(x) = 1 / (1 + e^(-x))

Tanh
(e^x - e^(-x))/(e^x + e^(-x))

Softsign
x/(x+|x|)

ReLU
f(u) = max(0,u)

ont-hot code
0 -> 1000 0000 00
1 -> 0100 0000 00
2 -> 0010 0000 00
3 -> 0001 0000 00
...
9 -> 0000 0000 01

MNIST data set
60000 * 28*28

Softmax fuc

softmax(x)i = exp(x)i/sum(exp(x)j)
example:
    output = [1, 5, 3]
    p1 = e^1/(e^1 + e^5 + e^3) 
    p2 = e^5/(e^1 + e^5 + e^3) 
    p3 = e^3/(e^1 + e^5 + e^3) 
    new_output = [p1,p2,p3]
